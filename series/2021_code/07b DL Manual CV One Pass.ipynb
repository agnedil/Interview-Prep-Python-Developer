{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL w/CV\n",
    "* [Source for this notebook](https://www.machinecurve.com/index.php/2020/02/18/how-to-use-k-fold-cross-validation-with-keras/)\n",
    "* [TIME SERIES FORCASTING WITH TENSORFLOW FROM THE AUTHORS!](https://www.tensorflow.org/tutorials/structured_data/time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.metrics import mean_squared_error as mse\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Bidirectional, LSTM, Dropout, ConvLSTM2D, Flatten, TimeDistributed, RepeatVector\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from helper import ( prepare_data, train_test_shuffle_split, train_test_seq_split, print_folds_stats )\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')     # 'fivethirtyeight'\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "#mae = tf.keras.losses.MeanAbsoluteError()\n",
    "#mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
    "#msle = tf.keras.losses.MeanSquaredLogarithmicError()        # square(log(y_true + 1.) - log(y_pred + 1.))\n",
    "#cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)      # reduction=tf.keras.losses.Reduction.SUM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare\n",
    "If the number of features is more than one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of loaded data: (2073, 8) \n",
      "\n",
      "Dropping 1 last row for golden data point\n",
      "Data prepared:\n",
      "\tSize of X = (2068 by 4)\n",
      "\tSize of y = (2068 by 4)\n",
      "\n",
      "TimeSeriesSplit indices:\n",
      "(Train, test): (348, 344)\n",
      "(Train, test): (692, 344)\n",
      "(Train, test): (1036, 344)\n",
      "(Train, test): (1380, 344)\n",
      "(Train, test): (1654, 344)\n",
      "\n",
      "KFold indices:\n",
      "(Train, test): (1654, 414)\n",
      "(Train, test): (1654, 414)\n",
      "(Train, test): (1654, 414)\n",
      "(Train, test): (1655, 413)\n",
      "(Train, test): (1655, 413)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv( 'data/2step_20210329.csv', encoding='utf-8' )\n",
    "print('Shape of loaded data:', df.shape, '\\n')\n",
    "\n",
    "n_steps      = 4\n",
    "random_state = 34\n",
    "features     = ['num1', 'num2', 'num3', 'num4']\n",
    "n_features   = len(features)\n",
    "\n",
    "( X,\n",
    "  y,\n",
    "  goldenx,\n",
    "  goldeny )  = prepare_data( df,\n",
    "                             features,\n",
    "                             n_steps,\n",
    "                             with_intersection=True,\n",
    "                             flatten=False )\n",
    "\n",
    "tscv = TimeSeriesSplit( n_splits=5, max_train_size=int( len(X)*0.8 ) )                    # no random_state\n",
    "\n",
    "X_sh, y_sh = deepcopy(X), deepcopy(y)\n",
    "X_sh, y_sh = shuffle( X_sh, y_sh, random_state=random_state, n_samples=None )\n",
    "kf = KFold( n_splits=5, shuffle=True, random_state=random_state )\n",
    "\n",
    "print_folds_stats( X, X_sh, tscv, kf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRY 1 BiLSTM LAYER, DROPOUT() AS A SEPARATE LAYER  \n",
    "metrics = [ 'mse', 'mae', 'mape', 'msle', 'cosine_similarity' ]  \n",
    "do not use sigmoid or tanh!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_BiLSTM( n_steps, n_features,\n",
    "                    learning_rate=0.005,\n",
    "                    units=750,\n",
    "                    activation='relu',\n",
    "                    dropout1=0,\n",
    "                    dropout2=0,\n",
    "                    dropout3=0,\n",
    "                    optimizer='adam',\n",
    "                    loss=mse,\n",
    "                    print_architecture=False ):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add( Bidirectional( LSTM( units, activation=activation,\n",
    "                                    dropout=dropout1,\n",
    "                                    recurrent_dropout=dropout2,\n",
    "                                    return_sequences=True),\n",
    "                                                          \n",
    "                              input_shape=(n_steps, n_features)\n",
    "                            )\n",
    "             )\n",
    "        \n",
    "    model.add( Bidirectional( LSTM( units, activation=activation ) ) )\n",
    "    model.add(Dropout( dropout3 ))\n",
    "    model.add( Dense( n_features ) )\n",
    "    \n",
    "    # 'mse', mae', 'mape', 'msle', 'cosine_similarity'\n",
    "    model.compile( optimizer=Adam(lr=learning_rate),\n",
    "                   loss=loss,\n",
    "                   metrics=[tf.keras.metrics.LogCoshError()] )     # metrics=['mse'], optimizer=Adam(lr=learning_rate)\n",
    "        \n",
    "    if print_architecture:\n",
    "        print(model.summary())\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_LSTM(   n_steps, n_features,\n",
    "                    learning_rate=0.005,\n",
    "                    units=750,\n",
    "                    activation='relu',\n",
    "                    dropout1=0,\n",
    "                    dropout2=0,\n",
    "                    dropout3=0,\n",
    "                    optimizer='adam',\n",
    "                    loss=mse,\n",
    "                    print_architecture=False ):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add( LSTM( units, activation=activation,\n",
    "                                    dropout=dropout1,\n",
    "                                    recurrent_dropout=dropout2,\n",
    "                                    return_sequences=True,\n",
    "                                                          \n",
    "                              input_shape=(n_steps, n_features)\n",
    "             ))\n",
    "        \n",
    "    model.add( LSTM( units, activation=activation ) )\n",
    "    model.add(Dropout( dropout3 ))\n",
    "    model.add( Dense( n_features ) )\n",
    "    \n",
    "    # 'mse', mae', 'mape', 'msle', 'cosine_similarity'\n",
    "    model.compile( optimizer=Adam(lr=learning_rate),\n",
    "                   loss=loss,\n",
    "                   metrics=[tf.keras.metrics.LogCoshError()] )     # metrics=['mse'], optimizer=Adam(lr=learning_rate)\n",
    "        \n",
    "    if print_architecture:\n",
    "        print(model.summary())\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 4, 750)            2265000   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 750)               4503000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 3004      \n",
      "=================================================================\n",
      "Total params: 6,771,004\n",
      "Trainable params: 6,771,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# PRINT AS AN EXAMPLE\n",
    "model = stacked_LSTM( n_steps, n_features, print_architecture=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "\n",
      "Training in fold 1 ...\n",
      "Epoch 1/6\n",
      "52/52 - 10s - loss: 48740.1094 - logcosh: 39.7523\n",
      "Epoch 2/6\n",
      "52/52 - 10s - loss: 108.4679 - logcosh: 8.2609\n",
      "Epoch 3/6\n",
      "52/52 - 10s - loss: 105.0282 - logcosh: 8.1604\n",
      "Epoch 4/6\n",
      "52/52 - 10s - loss: 104.3610 - logcosh: 8.1390\n",
      "Epoch 5/6\n",
      "52/52 - 10s - loss: 104.4013 - logcosh: 8.1488\n",
      "Epoch 6/6\n",
      "52/52 - 10s - loss: 104.3441 - logcosh: 8.1158\n",
      "13/13 - 1s - loss: 107.9290 - logcosh: 8.3244\n",
      "Score in fold 1: loss of 107.92896270751953; logcosh of 8.324374198913574\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Training in fold 2 ...\n",
      "Epoch 1/6\n",
      "52/52 - 10s - loss: 77835.2656 - logcosh: 48.8347\n",
      "Epoch 2/6\n",
      "52/52 - 11s - loss: 108.4825 - logcosh: 8.3046\n",
      "Epoch 3/6\n",
      "52/52 - 11s - loss: 106.3604 - logcosh: 8.2258\n",
      "Epoch 4/6\n",
      "52/52 - 10s - loss: 106.0916 - logcosh: 8.2426\n",
      "Epoch 5/6\n",
      "52/52 - 11s - loss: 106.8620 - logcosh: 8.2608\n",
      "Epoch 6/6\n",
      "52/52 - 10s - loss: 105.4269 - logcosh: 8.2126\n",
      "13/13 - 1s - loss: 103.1002 - logcosh: 8.0581\n",
      "Score in fold 2: loss of 103.10015869140625; logcosh of 8.058141708374023\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Training in fold 3 ...\n",
      "Epoch 1/6\n",
      "52/52 - 10s - loss: 47042.4492 - logcosh: 40.1710\n",
      "Epoch 2/6\n",
      "52/52 - 11s - loss: 110.4860 - logcosh: 8.3517\n",
      "Epoch 3/6\n",
      "52/52 - 11s - loss: 107.2694 - logcosh: 8.2421\n",
      "Epoch 4/6\n",
      "52/52 - 10s - loss: 106.6036 - logcosh: 8.2332\n",
      "Epoch 5/6\n",
      "52/52 - 10s - loss: 104.3628 - logcosh: 8.1417\n",
      "Epoch 6/6\n",
      "52/52 - 11s - loss: 106.6477 - logcosh: 8.2245\n",
      "13/13 - 1s - loss: 103.5389 - logcosh: 8.1629\n",
      "Score in fold 3: loss of 103.53885650634766; logcosh of 8.162931442260742\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Training in fold 4 ...\n",
      "Epoch 1/6\n",
      "52/52 - 9s - loss: 84528.2188 - logcosh: 49.6474\n",
      "Epoch 2/6\n",
      "52/52 - 10s - loss: 106.1431 - logcosh: 8.2131\n",
      "Epoch 3/6\n",
      "52/52 - 10s - loss: 104.7760 - logcosh: 8.1559\n",
      "Epoch 4/6\n",
      "52/52 - 11s - loss: 104.9006 - logcosh: 8.1507\n",
      "Epoch 5/6\n",
      "52/52 - 11s - loss: 104.7102 - logcosh: 8.1525\n",
      "Epoch 6/6\n",
      "52/52 - 10s - loss: 106.2989 - logcosh: 8.1992\n",
      "13/13 - 1s - loss: 106.6171 - logcosh: 8.2585\n",
      "Score in fold 4: loss of 106.61714172363281; logcosh of 8.25853157043457\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Training in fold 5 ...\n",
      "Epoch 1/6\n",
      "52/52 - 10s - loss: 38887.0078 - logcosh: 36.8247\n",
      "Epoch 2/6\n",
      "52/52 - 10s - loss: 108.5400 - logcosh: 8.2931\n",
      "Epoch 3/6\n",
      "52/52 - 10s - loss: 106.2592 - logcosh: 8.2195\n",
      "Epoch 4/6\n",
      "52/52 - 10s - loss: 106.1110 - logcosh: 8.2232\n",
      "Epoch 5/6\n",
      "52/52 - 10s - loss: 106.5929 - logcosh: 8.2285\n",
      "Epoch 6/6\n",
      "52/52 - 10s - loss: 105.3184 - logcosh: 8.1892\n",
      "13/13 - 1s - loss: 103.2017 - logcosh: 8.1058\n",
      "Score in fold 5: loss of 103.20169067382812; logcosh of 8.10583782196045\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs     = 6\n",
    "verbose    = 2\n",
    "score_per_fold, loss_per_fold = [], []\n",
    "\n",
    "fold_no = 1\n",
    "for train_idx, test_idx in kf.split( X_sh ):\n",
    "\n",
    "    # Define and compile model\n",
    "    model = stacked_BiLSTM( n_steps, n_features )\n",
    "    \n",
    "    print('\\n', '='*100, '\\n', sep='')\n",
    "    print('Training in fold {} ...'.format( fold_no ))\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit( X_sh[train_idx].astype(np.float), y_sh[train_idx].astype(np.float),\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=epochs,\n",
    "                         #shuffle=False,\n",
    "                         verbose=verbose )\n",
    "\n",
    "    # Generate metrics\n",
    "    scores = model.evaluate( X_sh[test_idx].astype(np.float), y_sh[test_idx].astype(np.float), verbose=verbose )\n",
    "    print(f'Score in fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]}')\n",
    "    score_per_fold.append( scores[1] )\n",
    "    loss_per_fold.append(  scores[0] )\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "Per fold:\n",
      "              1         2         3         4         5     \n",
      "Loss       107.929   103.1002  103.5389  106.6171  103.2017 \n",
      "Score       8.3244    8.0581    8.1629    8.2585    8.1058  \n",
      "====================================================================================================\n",
      "Average loss:  104.8774  +-2.0048\n",
      "Average score: 8.182\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Average scores\n",
    "print( '\\n', '='*100, sep='' )\n",
    "print('Per fold:')\n",
    "print( '{:>10}{:^10}{:^10}{:^10}{:^10}{:^10}'.format('', 1, 2, 3, 4, 5) )\n",
    "print( '{:<10}{:^10}{:^10}{:^10}{:^10}{:^10}'.format( 'Loss', *[ round(i, 4) for i in loss_per_fold ] ) )\n",
    "print( '{:<10}{:^10}{:^10}{:^10}{:^10}{:^10}'.format( 'Score', *[ round(i, 4) for i in score_per_fold ] ) )\n",
    "print( '='*100 )\n",
    "print(f'Average loss:  {round( np.mean(loss_per_fold), 4 )}  +-{round( np.std(loss_per_fold), 4 )}')\n",
    "print(f'Average score: {round( np.mean(score_per_fold), 4)}')\n",
    "print( '='*100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-169098488ee7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MSE scores:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Log cosh scores:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "y1 = [i[0] for i in metrics[:-12]]\n",
    "y2 = [i[1] for i in metrics[:-12]]\n",
    "print('MSE scores:', y1)\n",
    "print('Log cosh scores:', y2)\n",
    "x = range(0,len(y1))\n",
    "xtick_labels = [i[2] for i in metrics[:-12]]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(y1)\n",
    "plt.xticks(x, xtick_labels)\n",
    "plt.title('HP Gridsearch: Epochs at Batch Size = 8 (TimeSeries)')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(y2)\n",
    "plt.xticks(x, xtick_labels)\n",
    "plt.title('HP Gridsearch: Epochs at Batch Size = 8 (TimeSeries)')\n",
    "plt.ylabel('Logcosh')\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
