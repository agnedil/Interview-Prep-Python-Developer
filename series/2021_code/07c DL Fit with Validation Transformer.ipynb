{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL Model Fit with Validation (Transformer)\n",
    "* Test several loss functions and optimizers\n",
    "* Use several metrics to compare\n",
    "* Test on last data point\n",
    "* Visualize loss and metrics\n",
    "* Evaluate and predict on train set to try to foresee the result\n",
    "\n",
    "__If any single HP has to be tuned, this should be the learning rate__. Good articles about learning rate:  \n",
    "a) Theory: https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/  \n",
    "b) Practice: https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/   \n",
    "FINDING THE RIGHT LEARNING SHOULD A WHOLE SEPARATE EXPERIMENT OF ITS OWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import RepeatVector, TimeDistributed, Input, BatchNormalization, \\\n",
    "    multiply, concatenate, Flatten, Activation, dot\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import plot_model, model_to_dot\n",
    "import pydot as pyd\n",
    "keras.utils.vis_utils.pydot = pyd\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import random\n",
    "import pickle as pkl\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Bidirectional, LSTM, Dropout, ConvLSTM2D, Flatten, TimeDistributed, RepeatVector\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adamax, Adam, Nadam, Ftrl\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from helper import ( prepare_data, train_test_shuffle_split, train_test_seq_split, print_folds_stats )\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')     # 'fivethirtyeight'\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSSES\n",
    "mse     = tf.keras.losses.MeanSquaredError()\n",
    "mae     = tf.keras.losses.MeanAbsoluteError()\n",
    "mape    = tf.keras.losses.MeanAbsolutePercentageError()\n",
    "msle    = tf.keras.losses.MeanSquaredLogarithmicError()        # square(log(y_true + 1.) - log(y_pred + 1.))\n",
    "huber   = tf.keras.losses.Huber()\n",
    "logcosh = tf.keras.losses.LogCosh()\n",
    "\n",
    "# METRICS\n",
    "metric_mse   = tf.keras.metrics.MeanSquaredError()\n",
    "metric_rmse  = tf.keras.metrics.RootMeanSquaredError()\n",
    "metric_mae   = tf.keras.metrics.MeanAbsoluteError()\n",
    "metric_mape  = tf.keras.metrics.MeanAbsolutePercentageError()\n",
    "metric_msle  = tf.keras.metrics.MeanSquaredLogarithmicError()\n",
    "metric_lcosh = tf.keras.metrics.LogCoshError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THESE GUYS SUPPOSEDLY HAVE IN-BUILT ADAPTIVE LRs\n",
    "# IT IS RECOMMENDED TO KEEP THE DEFAULT HYPERPARAMETERS UNCHANGED (I saw confirmations of this with lower starting\n",
    "# error scores for RMSprop)\n",
    "# Adam is improved RMSprop which is improved Adadelta which is improved Adagrad (latter - aggressive adaptive decay)\n",
    "# More details: https://keras.io/api/optimizers/\n",
    "\n",
    "#keras.optimizers.Adagrad(  lr=0.01,  epsilon=1e-08, decay=0.0 )\n",
    "#keras.optimizers.Adadelta( lr=1.0,   rho=0.95,      epsilon=1e-08, decay=0.0 )\n",
    "#keras.optimizers.RMSprop(  lr=0.001, rho=0.9,       epsilon=1e-08, decay=0.0 )\n",
    "#keras.optimizers.Adam(     lr=0.001, beta_1=0.9,    beta_2=0.999,  epsilon=1e-08, decay=0.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT ON ONE DATA POINT\n",
    "def predict_one(model, X_last, loss_func=mse):\n",
    "       \n",
    "    X_last = X_last.reshape((1, n_steps, n_features))\n",
    "    pred = model.predict(X_last, verbose=0)\n",
    "    return np.round( pred[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of loaded data: (2073, 8) \n",
      "\n",
      "Dropping 1 last row for golden data point\n",
      "Data prepared:\n",
      "\tSize of X = (2062 by 10)\n",
      "\tSize of y = (2062 by 4)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv( 'data/2step_20210329.csv', encoding='utf-8' )\n",
    "print('Shape of loaded data:', df.shape, '\\n')\n",
    "\n",
    "random_state      = 34\n",
    "n_steps           = 10\n",
    "features          = ['num1', 'num2', 'num3', 'num4']\n",
    "n_features        = len(features)\n",
    "n_output_features = len(features)\n",
    "with_intersection = True\n",
    "flatten           = False\n",
    "\n",
    "( X,\n",
    "  y,\n",
    "  goldenx,\n",
    "  goldeny )  = prepare_data( df,\n",
    "                             features,\n",
    "                             n_steps,\n",
    "                             with_intersection=with_intersection,\n",
    "                             flatten=flatten )\n",
    "\n",
    "X_sh, y_sh = deepcopy(X), deepcopy(y)\n",
    "X_sh, y_sh = shuffle( X_sh, y_sh, random_state=random_state, n_samples=None )\n",
    "\n",
    "# shape changes to [1, n_features * n_step], but still need to keep n_output_features for output layer\n",
    "if flatten:\n",
    "    n_features = n_features * n_steps\n",
    "    n_steps    = 1\n",
    "    X = X.reshape(X.shape[0], n_steps, n_features)\n",
    "    X_sh = X_sh.reshape(X.shape[0], n_steps, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[1]: [[30 34 23  4]\n",
      " [ 3 29 18 33]\n",
      " [23 35 28 17]\n",
      " [16 11 28  4]\n",
      " [32  4 28 34]\n",
      " [ 4  3 33  2]\n",
      " [21 10 31  6]\n",
      " [ 7 24 25 20]\n",
      " [30 25 16 23]\n",
      " [21 20 31  8]]\n",
      "y[1]: [23  3 17 28]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m</th>\n",
       "      <th>d</th>\n",
       "      <th>y</th>\n",
       "      <th>num1</th>\n",
       "      <th>num2</th>\n",
       "      <th>num3</th>\n",
       "      <th>num4</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>2001</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>2001</td>\n",
       "      <td>30</td>\n",
       "      <td>34</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>2001</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>33</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>2001</td>\n",
       "      <td>23</td>\n",
       "      <td>35</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>2001</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>2001</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>2001</td>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>2001</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>2001</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>2001</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2001</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2001</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>2001</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>32</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>2001</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>2001</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>2001</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>2001</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>2001</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>2001</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2001</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2001</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2001</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    m   d     y  num1  num2  num3  num4   b\n",
       "0   5  18  2001     9    10    22    13   1\n",
       "1   5  22  2001    30    34    23     4  13\n",
       "2   5  25  2001     3    29    18    33  17\n",
       "3   5  29  2001    23    35    28    17  20\n",
       "4   6   1  2001    16    11    28     4  31\n",
       "5   6   5  2001    32     4    28    34  20\n",
       "6   6   8  2001     4     3    33     2  29\n",
       "7   6  12  2001    21    10    31     6  21\n",
       "8   6  15  2001     7    24    25    20  35\n",
       "9   6  19  2001    30    25    16    23  25\n",
       "10  6  22  2001    21    20    31     8   4\n",
       "11  6  26  2001    23     3    17    28  22\n",
       "12  6  29  2001    33    30     9    15  28\n",
       "13  7   3  2001    26    27    11     8  11\n",
       "14  7   6  2001    14     4     6     2  35\n",
       "15  7  10  2001    12    22    32    18   2\n",
       "16  7  13  2001    32    23    19    14   9\n",
       "17  7  17  2001     6    27    20     9  29\n",
       "18  7  20  2001    16    10    12    35  11\n",
       "19  7  24  2001    18     1    16    19   5\n",
       "20  7  27  2001    21    30     2    33   6\n",
       "21  7  31  2001    30    17    11    16  33\n",
       "22  8   3  2001    12     8    11     2  20\n",
       "23  8   7  2001    28     9     3    11  24\n",
       "24  8  10  2001     8    17    16    18  35"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review data\n",
    "idx = 1\n",
    "print(f'X[{idx}]:', X[idx])\n",
    "print(f'y[{idx}]:', y[idx])\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2062, 10, 4)\n",
      "(2062, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "https://levelup.gitconnected.com/building-seq2seq-lstm-with-luong-attention-in-keras-for-time-series-forecasting-1ee00958decb  \n",
    "Note: there is some data preparation done in the beginning. __Relevant__?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Simple LSTM\n",
    "best result:\n",
    "epochs        = 250\n",
    "n_hidden      = 100\n",
    "learning_rate = 0.001\n",
    "es            = EarlyStopping( monitor='loss', mode='min', patience=50 )\n",
    "activation    = 'elu'\n",
    "opt           = Adam(lr=learning_rate)#, clipnorm=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs        = 250\n",
    "n_hidden      = 100\n",
    "learning_rate = 0.001\n",
    "es            = EarlyStopping( monitor='loss', mode='min', patience=50 )\n",
    "activation    = 'elu'\n",
    "opt           = Adam(lr=learning_rate)#, clipnorm=1)\n",
    "\n",
    "input_train = Input(shape=(X.shape[1], X.shape[2]))\n",
    "output_train = Input(shape=(y.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lstm_18/Identity:0\", shape=(None, 100), dtype=float32)\n",
      "Tensor(\"lstm_18/Identity_1:0\", shape=(None, 100), dtype=float32)\n",
      "Tensor(\"lstm_18/Identity_2:0\", shape=(None, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "encoder_last_h1, encoder_last_h2, encoder_last_c = LSTM(\n",
    " n_hidden, activation=activation, dropout=0.2, recurrent_dropout=0, \n",
    " return_sequences=False, return_state=True)(input_train)\n",
    "print(encoder_last_h1)\n",
    "print(encoder_last_h2)\n",
    "print(encoder_last_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_last_h1 = BatchNormalization(momentum=0.6)(encoder_last_h1)\n",
    "encoder_last_c = BatchNormalization(momentum=0.6)(encoder_last_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lstm_19/Identity:0\", shape=(None, 4, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "decoder = RepeatVector(y.shape[1])(encoder_last_h1)\n",
    "decoder = LSTM(n_hidden, activation=activation, dropout=0.2, recurrent_dropout=0, return_state=False, return_sequences=True)(\n",
    "    decoder, initial_state=[encoder_last_h1, encoder_last_c])\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"time_distributed_9/Identity:0\", shape=(None, 4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "out = TimeDistributed(Dense(1))(decoder)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, 10, 4)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_18 (LSTM)                  [(None, 100), (None, 42000       input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 100)          400         lstm_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_10 (RepeatVector) (None, 4, 100)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 100)          400         lstm_18[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  (None, 4, 100)       80400       repeat_vector_10[0][0]           \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, 4, 1)         101         lstm_19[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 123,301\n",
      "Trainable params: 122,901\n",
      "Non-trainable params: 400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=input_train, outputs=out)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "49/49 [==============================] - 2s 41ms/step - loss: 257.1178 - mae: 13.1991 - val_loss: 140.9627 - val_mae: 9.7517\n",
      "Epoch 2/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 123.0559 - mae: 9.3746 - val_loss: 112.5370 - val_mae: 9.0031\n",
      "Epoch 3/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 110.4242 - mae: 9.0049 - val_loss: 109.8082 - val_mae: 8.9500\n",
      "Epoch 4/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 108.5866 - mae: 8.9383 - val_loss: 106.4977 - val_mae: 8.8370\n",
      "Epoch 5/250\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 106.8916 - mae: 8.8797 - val_loss: 106.8510 - val_mae: 8.8693\n",
      "Epoch 6/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 106.3647 - mae: 8.8790 - val_loss: 105.3746 - val_mae: 8.8199\n",
      "Epoch 7/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 105.9240 - mae: 8.8641 - val_loss: 105.0402 - val_mae: 8.8219\n",
      "Epoch 8/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 105.9342 - mae: 8.8593 - val_loss: 106.4540 - val_mae: 8.8732\n",
      "Epoch 9/250\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 104.7837 - mae: 8.8251 - val_loss: 104.0535 - val_mae: 8.7820\n",
      "Epoch 10/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 105.3067 - mae: 8.8444 - val_loss: 104.5700 - val_mae: 8.7941\n",
      "Epoch 11/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 104.2511 - mae: 8.8001 - val_loss: 104.4337 - val_mae: 8.8153\n",
      "Epoch 12/250\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 104.8343 - mae: 8.8249 - val_loss: 104.0086 - val_mae: 8.8010\n",
      "Epoch 13/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 103.7043 - mae: 8.7789 - val_loss: 104.6730 - val_mae: 8.8131\n",
      "Epoch 14/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 104.2348 - mae: 8.8103 - val_loss: 104.0504 - val_mae: 8.8001\n",
      "Epoch 15/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 103.9989 - mae: 8.7959 - val_loss: 104.8458 - val_mae: 8.8168\n",
      "Epoch 16/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 103.7816 - mae: 8.7948 - val_loss: 104.9211 - val_mae: 8.8096\n",
      "Epoch 17/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 104.0602 - mae: 8.8015 - val_loss: 104.9619 - val_mae: 8.8264\n",
      "Epoch 18/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 103.1704 - mae: 8.7602 - val_loss: 106.0528 - val_mae: 8.8552\n",
      "Epoch 19/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 102.8178 - mae: 8.7443 - val_loss: 104.1510 - val_mae: 8.8040\n",
      "Epoch 20/250\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 103.0216 - mae: 8.7482 - val_loss: 104.4379 - val_mae: 8.8088\n",
      "Epoch 21/250\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 103.1201 - mae: 8.7556 - val_loss: 104.0525 - val_mae: 8.7902\n",
      "Epoch 22/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 102.5185 - mae: 8.7255 - val_loss: 105.6173 - val_mae: 8.8184\n",
      "Epoch 23/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 102.8744 - mae: 8.7455 - val_loss: 104.6145 - val_mae: 8.8047\n",
      "Epoch 24/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 102.8058 - mae: 8.7291 - val_loss: 105.8635 - val_mae: 8.8380\n",
      "Epoch 25/250\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 103.0658 - mae: 8.7666 - val_loss: 106.1279 - val_mae: 8.8325\n",
      "Epoch 26/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 102.3776 - mae: 8.7216 - val_loss: 105.2714 - val_mae: 8.8390\n",
      "Epoch 27/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 103.1296 - mae: 8.7366 - val_loss: 104.4241 - val_mae: 8.8071\n",
      "Epoch 28/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 101.5698 - mae: 8.6896 - val_loss: 105.2029 - val_mae: 8.8304\n",
      "Epoch 29/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 102.1278 - mae: 8.7177 - val_loss: 105.5933 - val_mae: 8.8480\n",
      "Epoch 30/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 101.4765 - mae: 8.6913 - val_loss: 106.2603 - val_mae: 8.8484\n",
      "Epoch 31/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 102.2332 - mae: 8.7221 - val_loss: 105.4045 - val_mae: 8.8126\n",
      "Epoch 32/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 101.7200 - mae: 8.6833 - val_loss: 104.4038 - val_mae: 8.7875\n",
      "Epoch 33/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 101.4491 - mae: 8.6515 - val_loss: 105.0909 - val_mae: 8.8172\n",
      "Epoch 34/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 102.2121 - mae: 8.6869 - val_loss: 104.6409 - val_mae: 8.8223\n",
      "Epoch 35/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 101.7781 - mae: 8.6856 - val_loss: 104.9812 - val_mae: 8.8230\n",
      "Epoch 36/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 101.8972 - mae: 8.6791 - val_loss: 106.3035 - val_mae: 8.8495\n",
      "Epoch 37/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 100.8029 - mae: 8.6379 - val_loss: 106.4146 - val_mae: 8.8463\n",
      "Epoch 38/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 101.5715 - mae: 8.6848 - val_loss: 105.0680 - val_mae: 8.8212\n",
      "Epoch 39/250\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 102.0500 - mae: 8.7151 - val_loss: 106.1675 - val_mae: 8.8556\n",
      "Epoch 40/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 101.2946 - mae: 8.6663 - val_loss: 105.6109 - val_mae: 8.8454\n",
      "Epoch 41/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 100.7996 - mae: 8.6399 - val_loss: 105.3974 - val_mae: 8.8181\n",
      "Epoch 42/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 100.6242 - mae: 8.6240 - val_loss: 106.9692 - val_mae: 8.8894\n",
      "Epoch 43/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 101.5358 - mae: 8.6836 - val_loss: 108.0721 - val_mae: 8.9129\n",
      "Epoch 44/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 101.5411 - mae: 8.6805 - val_loss: 107.4013 - val_mae: 8.9086\n",
      "Epoch 45/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 100.0947 - mae: 8.6004 - val_loss: 110.2710 - val_mae: 8.9698\n",
      "Epoch 46/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 101.4173 - mae: 8.6720 - val_loss: 105.6453 - val_mae: 8.8431\n",
      "Epoch 47/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 100.8051 - mae: 8.6402 - val_loss: 108.1072 - val_mae: 8.9179\n",
      "Epoch 48/250\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 100.2394 - mae: 8.6229 - val_loss: 106.6215 - val_mae: 8.9078\n",
      "Epoch 49/250\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 100.9286 - mae: 8.6543 - val_loss: 107.1491 - val_mae: 8.8903\n",
      "Epoch 50/250\n",
      "49/49 [==============================] - 1s 24ms/step - loss: 99.1828 - mae: 8.5683 - val_loss: 107.2918 - val_mae: 8.9036\n",
      "Epoch 51/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 99.6893 - mae: 8.5732 - val_loss: 106.6294 - val_mae: 8.8699\n",
      "Epoch 52/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 100.3448 - mae: 8.6018 - val_loss: 106.6193 - val_mae: 8.8648\n",
      "Epoch 53/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 99.3456 - mae: 8.5619 - val_loss: 107.4818 - val_mae: 8.9243\n",
      "Epoch 54/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 99.9519 - mae: 8.5629 - val_loss: 106.3348 - val_mae: 8.8449\n",
      "Epoch 55/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 99.3326 - mae: 8.5598 - val_loss: 105.1319 - val_mae: 8.8249\n",
      "Epoch 56/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 99.9673 - mae: 8.5911 - val_loss: 106.5691 - val_mae: 8.8490\n",
      "Epoch 57/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 98.8664 - mae: 8.5458 - val_loss: 105.1360 - val_mae: 8.8029\n",
      "Epoch 58/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 98.6229 - mae: 8.5255 - val_loss: 107.6625 - val_mae: 8.8996\n",
      "Epoch 59/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 98.6961 - mae: 8.5235 - val_loss: 107.0373 - val_mae: 8.8832\n",
      "Epoch 60/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 99.3682 - mae: 8.5785 - val_loss: 105.8982 - val_mae: 8.8475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 98.0604 - mae: 8.4989 - val_loss: 106.6648 - val_mae: 8.8710\n",
      "Epoch 62/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 98.7045 - mae: 8.5309 - val_loss: 107.8462 - val_mae: 8.9046\n",
      "Epoch 63/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 97.3627 - mae: 8.4777 - val_loss: 108.0747 - val_mae: 8.9239\n",
      "Epoch 64/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 98.1528 - mae: 8.5027 - val_loss: 108.1922 - val_mae: 8.9201\n",
      "Epoch 65/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 98.7418 - mae: 8.5259 - val_loss: 108.1347 - val_mae: 8.9047\n",
      "Epoch 66/250\n",
      "49/49 [==============================] - 1s 24ms/step - loss: 97.8136 - mae: 8.4826 - val_loss: 109.1110 - val_mae: 8.9356\n",
      "Epoch 67/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 97.9061 - mae: 8.4923 - val_loss: 111.3326 - val_mae: 9.0268\n",
      "Epoch 68/250\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 97.8285 - mae: 8.4783 - val_loss: 107.9996 - val_mae: 8.9138\n",
      "Epoch 69/250\n",
      "49/49 [==============================] - 2s 38ms/step - loss: 97.4517 - mae: 8.4558 - val_loss: 110.6241 - val_mae: 9.0063\n",
      "Epoch 70/250\n",
      "49/49 [==============================] - 4s 72ms/step - loss: 97.4674 - mae: 8.4388 - val_loss: 109.3792 - val_mae: 8.9809\n",
      "Epoch 71/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 97.5144 - mae: 8.4634 - val_loss: 107.2097 - val_mae: 8.9151\n",
      "Epoch 72/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 97.8529 - mae: 8.4795 - val_loss: 108.5425 - val_mae: 8.9313\n",
      "Epoch 73/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 96.6938 - mae: 8.4053 - val_loss: 109.9707 - val_mae: 8.9866\n",
      "Epoch 74/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 97.2029 - mae: 8.4467 - val_loss: 108.5657 - val_mae: 8.9554\n",
      "Epoch 75/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 96.8139 - mae: 8.4132 - val_loss: 108.4794 - val_mae: 8.9340\n",
      "Epoch 76/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 96.6680 - mae: 8.4200 - val_loss: 111.5269 - val_mae: 9.0364\n",
      "Epoch 77/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 96.3912 - mae: 8.3766 - val_loss: 107.7099 - val_mae: 8.9200\n",
      "Epoch 78/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 96.2697 - mae: 8.3899 - val_loss: 111.2634 - val_mae: 9.0366\n",
      "Epoch 79/250\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 96.3067 - mae: 8.4064 - val_loss: 110.9334 - val_mae: 9.0234\n",
      "Epoch 80/250\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 95.7881 - mae: 8.3824 - val_loss: 109.5350 - val_mae: 8.9757\n",
      "Epoch 81/250\n",
      "49/49 [==============================] - 2s 40ms/step - loss: 94.9822 - mae: 8.3266 - val_loss: 110.5426 - val_mae: 9.0114\n",
      "Epoch 82/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 95.9772 - mae: 8.3876 - val_loss: 109.2300 - val_mae: 8.9749\n",
      "Epoch 83/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 96.2223 - mae: 8.3597 - val_loss: 110.7132 - val_mae: 9.0377\n",
      "Epoch 84/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 95.9276 - mae: 8.3500 - val_loss: 111.3301 - val_mae: 9.0480\n",
      "Epoch 85/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 96.6066 - mae: 8.3930 - val_loss: 111.0227 - val_mae: 9.0197\n",
      "Epoch 86/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 95.0215 - mae: 8.3331 - val_loss: 110.2785 - val_mae: 8.9763\n",
      "Epoch 87/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 94.6467 - mae: 8.3197 - val_loss: 109.9722 - val_mae: 8.9857\n",
      "Epoch 88/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 95.7794 - mae: 8.3559 - val_loss: 110.4857 - val_mae: 8.9950\n",
      "Epoch 89/250\n",
      "49/49 [==============================] - 2s 45ms/step - loss: 94.6943 - mae: 8.3122 - val_loss: 114.4418 - val_mae: 9.0988\n",
      "Epoch 90/250\n",
      "49/49 [==============================] - 2s 36ms/step - loss: 94.6454 - mae: 8.2974 - val_loss: 109.7206 - val_mae: 9.0169\n",
      "Epoch 91/250\n",
      "49/49 [==============================] - 2s 38ms/step - loss: 93.4594 - mae: 8.2334 - val_loss: 111.2095 - val_mae: 9.0283\n",
      "Epoch 92/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 94.0757 - mae: 8.2563 - val_loss: 114.2232 - val_mae: 9.1465\n",
      "Epoch 93/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 93.9373 - mae: 8.2587 - val_loss: 113.8661 - val_mae: 9.1217\n",
      "Epoch 94/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 94.1172 - mae: 8.2796 - val_loss: 114.6098 - val_mae: 9.1538\n",
      "Epoch 95/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 94.4014 - mae: 8.2598 - val_loss: 111.6729 - val_mae: 9.0496\n",
      "Epoch 96/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 93.8338 - mae: 8.2688 - val_loss: 113.7552 - val_mae: 9.1304\n",
      "Epoch 97/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 92.2805 - mae: 8.1757 - val_loss: 113.1196 - val_mae: 9.0870\n",
      "Epoch 98/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 92.9150 - mae: 8.2196 - val_loss: 116.3589 - val_mae: 9.1909\n",
      "Epoch 99/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 93.3976 - mae: 8.2068 - val_loss: 112.3167 - val_mae: 9.0838\n",
      "Epoch 100/250\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 94.0852 - mae: 8.2429 - val_loss: 114.4731 - val_mae: 9.1329\n",
      "Epoch 101/250\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 92.1238 - mae: 8.1446 - val_loss: 113.5955 - val_mae: 9.1086\n",
      "Epoch 102/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 92.8829 - mae: 8.1873 - val_loss: 113.6783 - val_mae: 9.0879\n",
      "Epoch 103/250\n",
      "49/49 [==============================] - 2s 37ms/step - loss: 94.2892 - mae: 8.2460 - val_loss: 113.7749 - val_mae: 9.0921\n",
      "Epoch 104/250\n",
      "49/49 [==============================] - 2s 35ms/step - loss: 93.0293 - mae: 8.2216 - val_loss: 111.9570 - val_mae: 9.0725\n",
      "Epoch 105/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 93.1844 - mae: 8.2013 - val_loss: 111.5192 - val_mae: 9.0580\n",
      "Epoch 106/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 92.2974 - mae: 8.1828 - val_loss: 113.2294 - val_mae: 9.1016\n",
      "Epoch 107/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 91.7346 - mae: 8.1445 - val_loss: 113.3044 - val_mae: 9.1017\n",
      "Epoch 108/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 90.8061 - mae: 8.0862 - val_loss: 113.4872 - val_mae: 9.1194\n",
      "Epoch 109/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 92.0333 - mae: 8.1428 - val_loss: 113.7542 - val_mae: 9.1166\n",
      "Epoch 110/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 91.3339 - mae: 8.0888 - val_loss: 112.5728 - val_mae: 9.0793\n",
      "Epoch 111/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 91.2425 - mae: 8.0982 - val_loss: 114.1225 - val_mae: 9.1150\n",
      "Epoch 112/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 91.7926 - mae: 8.1106 - val_loss: 115.8343 - val_mae: 9.1831\n",
      "Epoch 113/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 92.1445 - mae: 8.1421 - val_loss: 115.5698 - val_mae: 9.1440\n",
      "Epoch 114/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 90.8610 - mae: 8.0784 - val_loss: 114.2338 - val_mae: 9.1143\n",
      "Epoch 115/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 90.8611 - mae: 8.0845 - val_loss: 115.4426 - val_mae: 9.1500\n",
      "Epoch 116/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 90.2296 - mae: 8.0433 - val_loss: 113.8836 - val_mae: 9.1385\n",
      "Epoch 117/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 90.1651 - mae: 8.0260 - val_loss: 115.6333 - val_mae: 9.2000\n",
      "Epoch 118/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 90.0127 - mae: 8.0142 - val_loss: 115.7550 - val_mae: 9.1784\n",
      "Epoch 119/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 89.0806 - mae: 7.9730 - val_loss: 116.9814 - val_mae: 9.2180\n",
      "Epoch 120/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 88.4295 - mae: 7.9772 - val_loss: 119.5904 - val_mae: 9.2847\n",
      "Epoch 121/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 1s 24ms/step - loss: 89.1723 - mae: 7.9883 - val_loss: 117.4815 - val_mae: 9.2346\n",
      "Epoch 122/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 89.6421 - mae: 7.9937 - val_loss: 115.3156 - val_mae: 9.1587\n",
      "Epoch 123/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 88.0458 - mae: 7.9398 - val_loss: 118.6053 - val_mae: 9.2350\n",
      "Epoch 124/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 88.9556 - mae: 7.9650 - val_loss: 116.9853 - val_mae: 9.2032\n",
      "Epoch 125/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 87.4203 - mae: 7.8915 - val_loss: 119.6473 - val_mae: 9.3240\n",
      "Epoch 126/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 88.5694 - mae: 7.9366 - val_loss: 119.1675 - val_mae: 9.2946\n",
      "Epoch 127/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 89.3947 - mae: 7.9935 - val_loss: 119.1832 - val_mae: 9.2840\n",
      "Epoch 128/250\n",
      "49/49 [==============================] - 1s 31ms/step - loss: 88.9799 - mae: 7.9776 - val_loss: 118.6325 - val_mae: 9.2524\n",
      "Epoch 129/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 88.2620 - mae: 7.9500 - val_loss: 116.1501 - val_mae: 9.1702\n",
      "Epoch 130/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 88.5837 - mae: 7.9411 - val_loss: 116.8923 - val_mae: 9.1940\n",
      "Epoch 131/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 89.6464 - mae: 7.9735 - val_loss: 118.4605 - val_mae: 9.2706\n",
      "Epoch 132/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 86.5199 - mae: 7.8533 - val_loss: 117.2182 - val_mae: 9.2395\n",
      "Epoch 133/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 87.3070 - mae: 7.8833 - val_loss: 120.0266 - val_mae: 9.3113\n",
      "Epoch 134/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 85.4720 - mae: 7.7920 - val_loss: 115.8652 - val_mae: 9.1971\n",
      "Epoch 135/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 86.7294 - mae: 7.8465 - val_loss: 124.5507 - val_mae: 9.4357\n",
      "Epoch 136/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 86.4872 - mae: 7.8307 - val_loss: 119.3607 - val_mae: 9.2903\n",
      "Epoch 137/250\n",
      "49/49 [==============================] - 1s 24ms/step - loss: 85.3771 - mae: 7.7666 - val_loss: 119.7881 - val_mae: 9.3165\n",
      "Epoch 138/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 85.4589 - mae: 7.7700 - val_loss: 118.9517 - val_mae: 9.2624\n",
      "Epoch 139/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 86.3188 - mae: 7.8406 - val_loss: 121.7611 - val_mae: 9.4095\n",
      "Epoch 140/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 86.7627 - mae: 7.8274 - val_loss: 118.6574 - val_mae: 9.2407\n",
      "Epoch 141/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 86.6005 - mae: 7.8411 - val_loss: 117.4233 - val_mae: 9.2265\n",
      "Epoch 142/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 86.1082 - mae: 7.8082 - val_loss: 116.2908 - val_mae: 9.2035\n",
      "Epoch 143/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 86.2303 - mae: 7.8249 - val_loss: 118.5550 - val_mae: 9.3084\n",
      "Epoch 144/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 85.3823 - mae: 7.7946 - val_loss: 123.4524 - val_mae: 9.3644\n",
      "Epoch 145/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 85.4399 - mae: 7.7691 - val_loss: 120.0063 - val_mae: 9.3008\n",
      "Epoch 146/250\n",
      "49/49 [==============================] - 2s 36ms/step - loss: 84.7568 - mae: 7.7044 - val_loss: 120.2037 - val_mae: 9.3315\n",
      "Epoch 147/250\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 85.2361 - mae: 7.7740 - val_loss: 121.4795 - val_mae: 9.3290\n",
      "Epoch 148/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 84.2934 - mae: 7.7013 - val_loss: 121.4311 - val_mae: 9.3425\n",
      "Epoch 149/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 85.0605 - mae: 7.7423 - val_loss: 120.6062 - val_mae: 9.3166\n",
      "Epoch 150/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 83.0259 - mae: 7.6333 - val_loss: 119.7289 - val_mae: 9.2973\n",
      "Epoch 151/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 84.5419 - mae: 7.7064 - val_loss: 121.5692 - val_mae: 9.3561\n",
      "Epoch 152/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 83.9156 - mae: 7.6756 - val_loss: 119.3297 - val_mae: 9.3128\n",
      "Epoch 153/250\n",
      "49/49 [==============================] - 1s 24ms/step - loss: 82.4566 - mae: 7.6130 - val_loss: 118.7779 - val_mae: 9.2594\n",
      "Epoch 154/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 82.5922 - mae: 7.5794 - val_loss: 123.2984 - val_mae: 9.3992\n",
      "Epoch 155/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 82.3687 - mae: 7.5831 - val_loss: 119.9534 - val_mae: 9.3042\n",
      "Epoch 156/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 83.8530 - mae: 7.6681 - val_loss: 121.2065 - val_mae: 9.3213\n",
      "Epoch 157/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 83.5249 - mae: 7.6579 - val_loss: 118.4219 - val_mae: 9.2683\n",
      "Epoch 158/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 84.2129 - mae: 7.6732 - val_loss: 122.1789 - val_mae: 9.3634\n",
      "Epoch 159/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 81.4171 - mae: 7.5383 - val_loss: 122.7026 - val_mae: 9.3687\n",
      "Epoch 160/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 83.9854 - mae: 7.6616 - val_loss: 120.8834 - val_mae: 9.3450\n",
      "Epoch 161/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 82.5332 - mae: 7.6122 - val_loss: 121.7678 - val_mae: 9.3644\n",
      "Epoch 162/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 82.1336 - mae: 7.5896 - val_loss: 122.7381 - val_mae: 9.3985\n",
      "Epoch 163/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 81.4626 - mae: 7.5477 - val_loss: 120.7854 - val_mae: 9.3239\n",
      "Epoch 164/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 80.9176 - mae: 7.5301 - val_loss: 122.2617 - val_mae: 9.3198\n",
      "Epoch 165/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 81.5666 - mae: 7.5137 - val_loss: 123.4411 - val_mae: 9.4034\n",
      "Epoch 166/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 80.4422 - mae: 7.4989 - val_loss: 120.7496 - val_mae: 9.3036\n",
      "Epoch 167/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 81.6023 - mae: 7.5238 - val_loss: 123.5664 - val_mae: 9.4467\n",
      "Epoch 168/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 79.1121 - mae: 7.4115 - val_loss: 119.3297 - val_mae: 9.3351\n",
      "Epoch 169/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 81.8050 - mae: 7.5503 - val_loss: 121.3343 - val_mae: 9.3389\n",
      "Epoch 170/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 80.4734 - mae: 7.4760 - val_loss: 120.4686 - val_mae: 9.3215\n",
      "Epoch 171/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 80.5609 - mae: 7.4746 - val_loss: 124.2277 - val_mae: 9.4670\n",
      "Epoch 172/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 80.1183 - mae: 7.4429 - val_loss: 121.0916 - val_mae: 9.3379\n",
      "Epoch 173/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 80.1891 - mae: 7.4270 - val_loss: 124.1647 - val_mae: 9.4301\n",
      "Epoch 174/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 80.9846 - mae: 7.5031 - val_loss: 124.5068 - val_mae: 9.4599\n",
      "Epoch 175/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 79.0771 - mae: 7.4250 - val_loss: 121.1857 - val_mae: 9.3241\n",
      "Epoch 176/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 79.1320 - mae: 7.4085 - val_loss: 121.8633 - val_mae: 9.3600\n",
      "Epoch 177/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 80.2657 - mae: 7.4578 - val_loss: 122.5878 - val_mae: 9.3521\n",
      "Epoch 178/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 78.9107 - mae: 7.4005 - val_loss: 121.1760 - val_mae: 9.3366\n",
      "Epoch 179/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 79.7807 - mae: 7.4549 - val_loss: 124.3286 - val_mae: 9.3931\n",
      "Epoch 180/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 80.6518 - mae: 7.4868 - val_loss: 123.7688 - val_mae: 9.3925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 78.6545 - mae: 7.3604 - val_loss: 121.5619 - val_mae: 9.3343\n",
      "Epoch 182/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 80.2395 - mae: 7.4540 - val_loss: 120.8615 - val_mae: 9.2910\n",
      "Epoch 183/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 79.1692 - mae: 7.3916 - val_loss: 121.4810 - val_mae: 9.3630\n",
      "Epoch 184/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 78.1633 - mae: 7.3659 - val_loss: 125.4520 - val_mae: 9.4422\n",
      "Epoch 185/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 78.2433 - mae: 7.3524 - val_loss: 121.0472 - val_mae: 9.3228\n",
      "Epoch 186/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 76.0823 - mae: 7.2238 - val_loss: 124.3187 - val_mae: 9.3823\n",
      "Epoch 187/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 78.0879 - mae: 7.3287 - val_loss: 125.1397 - val_mae: 9.4204\n",
      "Epoch 188/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 77.3358 - mae: 7.2831 - val_loss: 121.4653 - val_mae: 9.3003\n",
      "Epoch 189/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 77.1030 - mae: 7.2749 - val_loss: 121.7206 - val_mae: 9.3755\n",
      "Epoch 190/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 77.8465 - mae: 7.3122 - val_loss: 122.7334 - val_mae: 9.4018\n",
      "Epoch 191/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 77.2929 - mae: 7.2860 - val_loss: 123.9296 - val_mae: 9.4292\n",
      "Epoch 192/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 75.2331 - mae: 7.1741 - val_loss: 124.0246 - val_mae: 9.4268\n",
      "Epoch 193/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 75.1846 - mae: 7.1933 - val_loss: 122.3970 - val_mae: 9.3671\n",
      "Epoch 194/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 76.5207 - mae: 7.2194 - val_loss: 123.5475 - val_mae: 9.3877\n",
      "Epoch 195/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 79.6735 - mae: 7.3908 - val_loss: 126.4784 - val_mae: 9.4427\n",
      "Epoch 196/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 75.5814 - mae: 7.1980 - val_loss: 128.9822 - val_mae: 9.5182\n",
      "Epoch 197/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 75.6089 - mae: 7.1860 - val_loss: 123.6411 - val_mae: 9.3855\n",
      "Epoch 198/250\n",
      "49/49 [==============================] - 2s 36ms/step - loss: 76.1323 - mae: 7.2358 - val_loss: 127.4730 - val_mae: 9.4802\n",
      "Epoch 199/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 77.0382 - mae: 7.2754 - val_loss: 129.7389 - val_mae: 9.6431\n",
      "Epoch 200/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 76.3658 - mae: 7.2226 - val_loss: 124.6852 - val_mae: 9.4815\n",
      "Epoch 201/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 76.7510 - mae: 7.2329 - val_loss: 121.5395 - val_mae: 9.3169\n",
      "Epoch 202/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 75.9569 - mae: 7.2245 - val_loss: 125.1331 - val_mae: 9.4030\n",
      "Epoch 203/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 75.3316 - mae: 7.1965 - val_loss: 125.1385 - val_mae: 9.4271\n",
      "Epoch 204/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 74.2110 - mae: 7.1089 - val_loss: 126.8728 - val_mae: 9.4977\n",
      "Epoch 205/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 73.7738 - mae: 7.0689 - val_loss: 126.1916 - val_mae: 9.4262\n",
      "Epoch 206/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 75.6182 - mae: 7.1572 - val_loss: 123.7571 - val_mae: 9.3743\n",
      "Epoch 207/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 74.2526 - mae: 7.1065 - val_loss: 124.1566 - val_mae: 9.3912\n",
      "Epoch 208/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 75.7102 - mae: 7.1661 - val_loss: 125.3717 - val_mae: 9.4167\n",
      "Epoch 209/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 73.6615 - mae: 7.0976 - val_loss: 130.0145 - val_mae: 9.5876\n",
      "Epoch 210/250\n",
      "49/49 [==============================] - 1s 24ms/step - loss: 73.9924 - mae: 7.1181 - val_loss: 124.5236 - val_mae: 9.4128\n",
      "Epoch 211/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 73.9413 - mae: 7.0979 - val_loss: 126.4475 - val_mae: 9.4592\n",
      "Epoch 212/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 74.8455 - mae: 7.1392 - val_loss: 127.9291 - val_mae: 9.5253\n",
      "Epoch 213/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 75.1840 - mae: 7.1682 - val_loss: 123.8876 - val_mae: 9.4448\n",
      "Epoch 214/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 74.1875 - mae: 7.1268 - val_loss: 127.5413 - val_mae: 9.4974\n",
      "Epoch 215/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 74.4361 - mae: 7.1087 - val_loss: 123.8626 - val_mae: 9.3929\n",
      "Epoch 216/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 72.8427 - mae: 7.0145 - val_loss: 125.7176 - val_mae: 9.4480\n",
      "Epoch 217/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 71.9852 - mae: 6.9554 - val_loss: 129.3255 - val_mae: 9.5413\n",
      "Epoch 218/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 73.5416 - mae: 7.0710 - val_loss: 126.8042 - val_mae: 9.4691\n",
      "Epoch 219/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 71.2179 - mae: 6.8907 - val_loss: 127.4686 - val_mae: 9.4845\n",
      "Epoch 220/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 72.2556 - mae: 6.9652 - val_loss: 124.1225 - val_mae: 9.3700\n",
      "Epoch 221/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 70.9278 - mae: 6.9548 - val_loss: 128.3734 - val_mae: 9.5313\n",
      "Epoch 222/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 73.1593 - mae: 7.0267 - val_loss: 127.5081 - val_mae: 9.4589\n",
      "Epoch 223/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 70.9666 - mae: 6.9184 - val_loss: 128.4784 - val_mae: 9.4789\n",
      "Epoch 224/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 72.0024 - mae: 6.9797 - val_loss: 126.9053 - val_mae: 9.5101\n",
      "Epoch 225/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 73.9392 - mae: 7.0811 - val_loss: 128.3973 - val_mae: 9.5392\n",
      "Epoch 226/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 71.1989 - mae: 6.9433 - val_loss: 127.1379 - val_mae: 9.4745\n",
      "Epoch 227/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 69.5336 - mae: 6.8029 - val_loss: 126.6583 - val_mae: 9.4488\n",
      "Epoch 228/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 70.6192 - mae: 6.9121 - val_loss: 126.8486 - val_mae: 9.4652\n",
      "Epoch 229/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 72.5989 - mae: 7.0249 - val_loss: 127.6650 - val_mae: 9.5205\n",
      "Epoch 230/250\n",
      "49/49 [==============================] - 4s 74ms/step - loss: 71.7038 - mae: 6.9205 - val_loss: 132.1944 - val_mae: 9.6304\n",
      "Epoch 231/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 70.3268 - mae: 6.8773 - val_loss: 127.4311 - val_mae: 9.4830\n",
      "Epoch 232/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 71.3845 - mae: 6.9227 - val_loss: 124.6423 - val_mae: 9.4034\n",
      "Epoch 233/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 69.2800 - mae: 6.8247 - val_loss: 128.9762 - val_mae: 9.5733\n",
      "Epoch 234/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 69.2820 - mae: 6.8115 - val_loss: 127.4854 - val_mae: 9.4732\n",
      "Epoch 235/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 69.4875 - mae: 6.8093 - val_loss: 129.5110 - val_mae: 9.5583\n",
      "Epoch 236/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 69.5679 - mae: 6.7985 - val_loss: 126.9562 - val_mae: 9.4526\n",
      "Epoch 237/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 70.4677 - mae: 6.8623 - val_loss: 128.0560 - val_mae: 9.5023\n",
      "Epoch 238/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 69.8367 - mae: 6.8218 - val_loss: 127.7556 - val_mae: 9.4508\n",
      "Epoch 239/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 71.6006 - mae: 6.9055 - val_loss: 129.8835 - val_mae: 9.5757\n",
      "Epoch 240/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 70.5766 - mae: 6.8664 - val_loss: 126.9537 - val_mae: 9.4992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 68.1835 - mae: 6.7673 - val_loss: 131.1106 - val_mae: 9.6360\n",
      "Epoch 242/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 69.7423 - mae: 6.8211 - val_loss: 125.8749 - val_mae: 9.4576\n",
      "Epoch 243/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 70.0591 - mae: 6.8215 - val_loss: 128.4062 - val_mae: 9.5033\n",
      "Epoch 244/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 67.0286 - mae: 6.6949 - val_loss: 128.3133 - val_mae: 9.5458\n",
      "Epoch 245/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 69.3272 - mae: 6.7891 - val_loss: 131.4285 - val_mae: 9.6103\n",
      "Epoch 246/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 69.2992 - mae: 6.8255 - val_loss: 129.4509 - val_mae: 9.5615\n",
      "Epoch 247/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 69.3560 - mae: 6.7850 - val_loss: 128.1416 - val_mae: 9.4848\n",
      "Epoch 248/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 67.7475 - mae: 6.7626 - val_loss: 126.4724 - val_mae: 9.4430\n",
      "Epoch 249/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 68.5795 - mae: 6.7605 - val_loss: 127.0817 - val_mae: 9.4359\n",
      "Epoch 250/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 68.2319 - mae: 6.7486 - val_loss: 129.7463 - val_mae: 9.5800\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, validation_split=0.25, \n",
    "                    epochs=epochs, verbose=1, callbacks=[es], \n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_mae = history.history['mae']\n",
    "valid_mae = history.history['val_mae'] \n",
    "#model.save('model_forecasting_seq2seq.h5')\n",
    "\n",
    "plt.plot(train_mae, label='train mae'), \n",
    "plt.plot(valid_mae, label='validation mae')\n",
    "plt.ylabel('mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('train vs. validation accuracy (mae)')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on train set...\n",
      "65/65 [==============================] - 1s 9ms/step - loss: 63.3131 - mae: 6.2753\n",
      "\n",
      "Train:  63.31311798095703\n",
      "Train MSE:     6.275275707244873\n",
      "Predicting on last 100 rows of train set:\n",
      "\n",
      "True:      [19  5  4 28]\n",
      "Predicted:[[21.]\n",
      " [21.]\n",
      " [17.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [33 22  3 30]\n",
      "Predicted:[[14.]\n",
      " [20.]\n",
      " [18.]\n",
      " [24.]]\n",
      "============================\n",
      "True:      [23  2 18  3]\n",
      "Predicted:[[21.]\n",
      " [12.]\n",
      " [18.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [23  1 15 29]\n",
      "Predicted:[[13.]\n",
      " [20.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [10  7 18 33]\n",
      "Predicted:[[17.]\n",
      " [10.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [18 13  4 35]\n",
      "Predicted:[[16.]\n",
      " [11.]\n",
      " [ 7.]\n",
      " [15.]]\n",
      "============================\n",
      "True:      [27 11 13 32]\n",
      "Predicted:[[10.]\n",
      " [24.]\n",
      " [ 9.]\n",
      " [14.]]\n",
      "============================\n",
      "True:      [19 23  5 28]\n",
      "Predicted:[[26.]\n",
      " [14.]\n",
      " [ 9.]\n",
      " [11.]]\n",
      "============================\n",
      "True:      [23 24 17 34]\n",
      "Predicted:[[21.]\n",
      " [19.]\n",
      " [ 9.]\n",
      " [21.]]\n",
      "============================\n",
      "True:      [30 26  8 20]\n",
      "Predicted:[[20.]\n",
      " [17.]\n",
      " [18.]\n",
      " [21.]]\n",
      "============================\n",
      "True:      [ 8 30 11  3]\n",
      "Predicted:[[10.]\n",
      " [17.]\n",
      " [ 8.]\n",
      " [19.]]\n",
      "============================\n",
      "True:      [ 6 12 19  5]\n",
      "Predicted:[[22.]\n",
      " [14.]\n",
      " [18.]\n",
      " [28.]]\n",
      "============================\n",
      "True:      [31 18  5  3]\n",
      "Predicted:[[28.]\n",
      " [17.]\n",
      " [15.]\n",
      " [21.]]\n",
      "============================\n",
      "True:      [ 8  5  3 10]\n",
      "Predicted:[[25.]\n",
      " [23.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [ 4 18 23  9]\n",
      "Predicted:[[19.]\n",
      " [19.]\n",
      " [19.]\n",
      " [23.]]\n",
      "============================\n",
      "True:      [ 3 18 24 14]\n",
      "Predicted:[[21.]\n",
      " [25.]\n",
      " [13.]\n",
      " [22.]]\n",
      "============================\n",
      "True:      [24  8 26 10]\n",
      "Predicted:[[24.]\n",
      " [17.]\n",
      " [23.]\n",
      " [25.]]\n",
      "============================\n",
      "True:      [21  1 24 13]\n",
      "Predicted:[[21.]\n",
      " [25.]\n",
      " [18.]\n",
      " [12.]]\n",
      "============================\n",
      "True:      [15 34 30  9]\n",
      "Predicted:[[18.]\n",
      " [19.]\n",
      " [18.]\n",
      " [26.]]\n",
      "============================\n",
      "True:      [22  1 27 15]\n",
      "Predicted:[[24.]\n",
      " [30.]\n",
      " [25.]\n",
      " [19.]]\n",
      "============================\n",
      "True:      [23  6 32 27]\n",
      "Predicted:[[34.]\n",
      " [26.]\n",
      " [18.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [11  5 31 16]\n",
      "Predicted:[[23.]\n",
      " [26.]\n",
      " [18.]\n",
      " [20.]]\n",
      "============================\n",
      "True:      [18  2 19 25]\n",
      "Predicted:[[11.]\n",
      " [16.]\n",
      " [19.]\n",
      " [22.]]\n",
      "============================\n",
      "True:      [14 18 24 17]\n",
      "Predicted:[[16.]\n",
      " [20.]\n",
      " [22.]\n",
      " [20.]]\n",
      "============================\n",
      "True:      [ 4 30  1 22]\n",
      "Predicted:[[21.]\n",
      " [13.]\n",
      " [11.]\n",
      " [ 8.]]\n",
      "============================\n",
      "True:      [ 7 15  1 21]\n",
      "Predicted:[[18.]\n",
      " [19.]\n",
      " [17.]\n",
      " [10.]]\n",
      "============================\n",
      "True:      [32 25 23 30]\n",
      "Predicted:[[17.]\n",
      " [19.]\n",
      " [15.]\n",
      " [15.]]\n",
      "============================\n",
      "True:      [ 5  6 25 29]\n",
      "Predicted:[[20.]\n",
      " [25.]\n",
      " [ 5.]\n",
      " [15.]]\n",
      "============================\n",
      "True:      [13 32  9 29]\n",
      "Predicted:[[20.]\n",
      " [19.]\n",
      " [ 8.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [25 22 18 19]\n",
      "Predicted:[[16.]\n",
      " [22.]\n",
      " [ 9.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [ 5 25 22 28]\n",
      "Predicted:[[29.]\n",
      " [12.]\n",
      " [18.]\n",
      " [13.]]\n",
      "============================\n",
      "True:      [34 30 25 20]\n",
      "Predicted:[[21.]\n",
      " [22.]\n",
      " [20.]\n",
      " [20.]]\n",
      "============================\n",
      "True:      [ 4 17 14 18]\n",
      "Predicted:[[12.]\n",
      " [23.]\n",
      " [15.]\n",
      " [27.]]\n",
      "============================\n",
      "True:      [35 20 23 17]\n",
      "Predicted:[[17.]\n",
      " [12.]\n",
      " [28.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [ 4  1 34 13]\n",
      "Predicted:[[18.]\n",
      " [20.]\n",
      " [12.]\n",
      " [21.]]\n",
      "============================\n",
      "True:      [23 15 28  1]\n",
      "Predicted:[[12.]\n",
      " [17.]\n",
      " [15.]\n",
      " [25.]]\n",
      "============================\n",
      "True:      [27 29  3  8]\n",
      "Predicted:[[15.]\n",
      " [16.]\n",
      " [24.]\n",
      " [20.]]\n",
      "============================\n",
      "True:      [ 3 24 28 26]\n",
      "Predicted:[[26.]\n",
      " [17.]\n",
      " [17.]\n",
      " [26.]]\n",
      "============================\n",
      "True:      [ 4  8 11 27]\n",
      "Predicted:[[12.]\n",
      " [23.]\n",
      " [16.]\n",
      " [29.]]\n",
      "============================\n",
      "True:      [10 11 21 28]\n",
      "Predicted:[[21.]\n",
      " [20.]\n",
      " [13.]\n",
      " [23.]]\n",
      "============================\n",
      "True:      [16 10 13 20]\n",
      "Predicted:[[16.]\n",
      " [28.]\n",
      " [10.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [34 35  9 29]\n",
      "Predicted:[[25.]\n",
      " [14.]\n",
      " [18.]\n",
      " [23.]]\n",
      "============================\n",
      "True:      [19  3 27 29]\n",
      "Predicted:[[24.]\n",
      " [17.]\n",
      " [15.]\n",
      " [22.]]\n",
      "============================\n",
      "True:      [ 4 19 14 23]\n",
      "Predicted:[[24.]\n",
      " [20.]\n",
      " [17.]\n",
      " [11.]]\n",
      "============================\n",
      "True:      [13 32  2 34]\n",
      "Predicted:[[16.]\n",
      " [16.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [35  5  7 30]\n",
      "Predicted:[[21.]\n",
      " [23.]\n",
      " [ 5.]\n",
      " [19.]]\n",
      "============================\n",
      "True:      [ 1 28 32  4]\n",
      "Predicted:[[19.]\n",
      " [19.]\n",
      " [15.]\n",
      " [10.]]\n",
      "============================\n",
      "True:      [11 34 29 25]\n",
      "Predicted:[[15.]\n",
      " [22.]\n",
      " [17.]\n",
      " [21.]]\n",
      "============================\n",
      "True:      [ 1 15 18 22]\n",
      "Predicted:[[21.]\n",
      " [24.]\n",
      " [19.]\n",
      " [23.]]\n",
      "============================\n",
      "True:      [12 35 25  4]\n",
      "Predicted:[[28.]\n",
      " [21.]\n",
      " [18.]\n",
      " [15.]]\n",
      "============================\n",
      "True:      [21  7  9 14]\n",
      "Predicted:[[24.]\n",
      " [22.]\n",
      " [11.]\n",
      " [19.]]\n",
      "============================\n",
      "True:      [ 4  9 21 19]\n",
      "Predicted:[[21.]\n",
      " [15.]\n",
      " [19.]\n",
      " [20.]]\n",
      "============================\n",
      "True:      [20 12 14 34]\n",
      "Predicted:[[21.]\n",
      " [28.]\n",
      " [13.]\n",
      " [26.]]\n",
      "============================\n",
      "True:      [29 34  6 23]\n",
      "Predicted:[[19.]\n",
      " [16.]\n",
      " [24.]\n",
      " [25.]]\n",
      "============================\n",
      "True:      [ 6 13  9 27]\n",
      "Predicted:[[21.]\n",
      " [23.]\n",
      " [16.]\n",
      " [20.]]\n",
      "============================\n",
      "True:      [29 26 10  1]\n",
      "Predicted:[[19.]\n",
      " [17.]\n",
      " [ 8.]\n",
      " [22.]]\n",
      "============================\n",
      "True:      [23 17  9 19]\n",
      "Predicted:[[15.]\n",
      " [12.]\n",
      " [18.]\n",
      " [13.]]\n",
      "============================\n",
      "True:      [15 28 32 24]\n",
      "Predicted:[[26.]\n",
      " [19.]\n",
      " [11.]\n",
      " [21.]]\n",
      "============================\n",
      "True:      [11  5  8 10]\n",
      "Predicted:[[19.]\n",
      " [32.]\n",
      " [19.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [21 24 28  4]\n",
      "Predicted:[[21.]\n",
      " [15.]\n",
      " [20.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [ 6 18  4 19]\n",
      "Predicted:[[29.]\n",
      " [21.]\n",
      " [ 9.]\n",
      " [ 7.]]\n",
      "============================\n",
      "True:      [28 32 15  8]\n",
      "Predicted:[[28.]\n",
      " [24.]\n",
      " [20.]\n",
      " [22.]]\n",
      "============================\n",
      "True:      [26 32 10 21]\n",
      "Predicted:[[20.]\n",
      " [23.]\n",
      " [13.]\n",
      " [19.]]\n",
      "============================\n",
      "True:      [ 3  2 18  7]\n",
      "Predicted:[[13.]\n",
      " [23.]\n",
      " [25.]\n",
      " [19.]]\n",
      "============================\n",
      "True:      [19 14 20 15]\n",
      "Predicted:[[14.]\n",
      " [19.]\n",
      " [17.]\n",
      " [20.]]\n",
      "============================\n",
      "True:      [32 21  4 29]\n",
      "Predicted:[[26.]\n",
      " [12.]\n",
      " [19.]\n",
      " [20.]]\n",
      "============================\n",
      "True:      [34  4 13 21]\n",
      "Predicted:[[26.]\n",
      " [14.]\n",
      " [ 7.]\n",
      " [12.]]\n",
      "============================\n",
      "True:      [25 23  9 29]\n",
      "Predicted:[[21.]\n",
      " [16.]\n",
      " [23.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [13  5 16 30]\n",
      "Predicted:[[18.]\n",
      " [19.]\n",
      " [21.]\n",
      " [ 9.]]\n",
      "============================\n",
      "True:      [ 8 26 20  3]\n",
      "Predicted:[[16.]\n",
      " [13.]\n",
      " [18.]\n",
      " [20.]]\n",
      "============================\n",
      "True:      [14 25  4  1]\n",
      "Predicted:[[19.]\n",
      " [20.]\n",
      " [21.]\n",
      " [13.]]\n",
      "============================\n",
      "True:      [34  9 22  3]\n",
      "Predicted:[[20.]\n",
      " [14.]\n",
      " [24.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [31 32 26  2]\n",
      "Predicted:[[22.]\n",
      " [30.]\n",
      " [15.]\n",
      " [ 8.]]\n",
      "============================\n",
      "True:      [ 7 33 25  5]\n",
      "Predicted:[[26.]\n",
      " [15.]\n",
      " [18.]\n",
      " [26.]]\n",
      "============================\n",
      "True:      [ 2 26 11 35]\n",
      "Predicted:[[31.]\n",
      " [24.]\n",
      " [18.]\n",
      " [28.]]\n",
      "============================\n",
      "True:      [19 12 15  7]\n",
      "Predicted:[[15.]\n",
      " [11.]\n",
      " [16.]\n",
      " [20.]]\n",
      "============================\n",
      "True:      [20 12 16 33]\n",
      "Predicted:[[21.]\n",
      " [16.]\n",
      " [14.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [ 6  1 25 11]\n",
      "Predicted:[[15.]\n",
      " [17.]\n",
      " [17.]\n",
      " [21.]]\n",
      "============================\n",
      "True:      [23 17 21  1]\n",
      "Predicted:[[16.]\n",
      " [23.]\n",
      " [21.]\n",
      " [24.]]\n",
      "============================\n",
      "True:      [ 4 31  8  1]\n",
      "Predicted:[[18.]\n",
      " [25.]\n",
      " [ 8.]\n",
      " [ 8.]]\n",
      "============================\n",
      "True:      [29 26 23 21]\n",
      "Predicted:[[23.]\n",
      " [20.]\n",
      " [12.]\n",
      " [20.]]\n",
      "============================\n",
      "True:      [ 5 18  1 28]\n",
      "Predicted:[[ 8.]\n",
      " [26.]\n",
      " [12.]\n",
      " [20.]]\n",
      "============================\n",
      "True:      [32  4  6 12]\n",
      "Predicted:[[24.]\n",
      " [10.]\n",
      " [ 5.]\n",
      " [10.]]\n",
      "============================\n",
      "True:      [29  4 24 20]\n",
      "Predicted:[[16.]\n",
      " [13.]\n",
      " [14.]\n",
      " [ 7.]]\n",
      "============================\n",
      "True:      [25 15  1 21]\n",
      "Predicted:[[20.]\n",
      " [24.]\n",
      " [18.]\n",
      " [19.]]\n",
      "============================\n",
      "True:      [25 17 20 12]\n",
      "Predicted:[[28.]\n",
      " [18.]\n",
      " [20.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [18  8  1 12]\n",
      "Predicted:[[22.]\n",
      " [21.]\n",
      " [28.]\n",
      " [25.]]\n",
      "============================\n",
      "True:      [26 25 18 23]\n",
      "Predicted:[[25.]\n",
      " [22.]\n",
      " [23.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [10 14  6  1]\n",
      "Predicted:[[26.]\n",
      " [13.]\n",
      " [22.]\n",
      " [26.]]\n",
      "============================\n",
      "True:      [18 26 11 30]\n",
      "Predicted:[[20.]\n",
      " [14.]\n",
      " [21.]\n",
      " [12.]]\n",
      "============================\n",
      "True:      [19  9 22  5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:[[19.]\n",
      " [19.]\n",
      " [12.]\n",
      " [15.]]\n",
      "============================\n",
      "True:      [25 35  7 30]\n",
      "Predicted:[[20.]\n",
      " [21.]\n",
      " [12.]\n",
      " [15.]]\n",
      "============================\n",
      "True:      [13  2 26 24]\n",
      "Predicted:[[20.]\n",
      " [11.]\n",
      " [22.]\n",
      " [32.]]\n",
      "============================\n",
      "True:      [ 3 15 22 30]\n",
      "Predicted:[[21.]\n",
      " [18.]\n",
      " [16.]\n",
      " [14.]]\n",
      "============================\n",
      "True:      [30  3 11 19]\n",
      "Predicted:[[20.]\n",
      " [28.]\n",
      " [14.]\n",
      " [13.]]\n",
      "============================\n",
      "True:      [28  6  3 30]\n",
      "Predicted:[[27.]\n",
      " [20.]\n",
      " [ 8.]\n",
      " [13.]]\n",
      "============================\n",
      "True:      [ 8  9 28  3]\n",
      "Predicted:[[18.]\n",
      " [23.]\n",
      " [10.]\n",
      " [13.]]\n",
      "============================\n",
      "True:      [34 15 12 27]\n",
      "Predicted:[[20.]\n",
      " [22.]\n",
      " [24.]\n",
      " [20.]]\n",
      "============================\n",
      "True:      [ 7 29 22 26]\n",
      "Predicted:[[19.]\n",
      " [24.]\n",
      " [20.]\n",
      " [14.]]\n",
      "============================\n",
      "True:      [32  4 30  3]\n",
      "Predicted:[[16.]\n",
      " [18.]\n",
      " [13.]\n",
      " [11.]]\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "def predict_on_trainset( model, X_, y_ ):\n",
    "    \n",
    "    for feature_set, target in zip( X_, y_ ):\n",
    "        print('True:     ', target)\n",
    "        print('Predicted:', predict_one( model, feature_set ), '\\n', '='*28, sep='')\n",
    "                \n",
    "        \n",
    "# evaluate\n",
    "print('Evaluating on train set...')\n",
    "score = model.evaluate( X, y, verbose=1 )\n",
    "print('\\nTrain: ', score[0])\n",
    "print('Train MSE:    ', score[1])\n",
    "\n",
    "# predict\n",
    "m = 100\n",
    "print(f'Predicting on last {m} rows of train set:\\n')\n",
    "predict_on_trainset( model, X[-m:], y[-m:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adam.Adam at 0x7f0df5031b50>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_15:0\", shape=(None, 10, 4), dtype=float32)\n",
      "Tensor(\"input_16:0\", shape=(None, 4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "epochs        = 250\n",
    "n_hidden      = 50\n",
    "learning_rate = 0.001\n",
    "es            = EarlyStopping( monitor='loss', mode='min', patience=50 )\n",
    "activation    = 'relu'\n",
    "opt           = Adamax(lr=learning_rate)#, clipnorm=1)\n",
    "\n",
    "input_train = Input(shape=(X.shape[1], X.shape[2]))\n",
    "output_train = Input(shape=(y.shape[1], 1))\n",
    "print(input_train)\n",
    "print(output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lstm_14/Identity:0\", shape=(None, 10, 50), dtype=float32)\n",
      "Tensor(\"lstm_14/Identity_1:0\", shape=(None, 50), dtype=float32)\n",
      "Tensor(\"lstm_14/Identity_2:0\", shape=(None, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "encoder_stack_h, encoder_last_h, encoder_last_c = LSTM(\n",
    "    n_hidden, activation=activation, dropout=0.2, recurrent_dropout=0, \n",
    "    return_state=True, return_sequences=True)(input_train)\n",
    "print(encoder_stack_h)\n",
    "print(encoder_last_h)\n",
    "print(encoder_last_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_last_h = BatchNormalization(momentum=0.6)(encoder_last_h)\n",
    "#encoder_last_c = BatchNormalization(momentum=0.6)(encoder_last_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"repeat_vector_8/Identity:0\", shape=(None, 4, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "decoder_input = RepeatVector(y.shape[1])(encoder_last_h)\n",
    "print(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lstm_15/Identity:0\", shape=(None, 4, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "decoder_stack_h = LSTM(n_hidden, activation=activation, dropout=0.2, recurrent_dropout=0,\n",
    " return_state=False, return_sequences=True)(\n",
    " decoder_input, initial_state=[encoder_last_h, encoder_last_c])\n",
    "print(decoder_stack_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"activation_4/Identity:0\", shape=(None, 4, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# ATTENTION LAYER\n",
    "attention = dot([decoder_stack_h, encoder_stack_h], axes=[2, 2])\n",
    "attention = Activation('softmax')(attention)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dot_9/Identity:0\", shape=(None, 4, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "context = dot([attention, encoder_stack_h], axes=[2,1])\n",
    "#context = BatchNormalization(momentum=0.8)(context)                 # originally 0.6\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_4/Identity:0\", shape=(None, 4, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "decoder_combined_context = concatenate([context, decoder_stack_h])\n",
    "print(decoder_combined_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"time_distributed_7/Identity:0\", shape=(None, 4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "out = TimeDistributed(Dense(1))(decoder_combined_context)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, 10, 4)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  [(None, 10, 50), (No 11000       input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_8 (RepeatVector)  (None, 4, 50)        0           lstm_14[0][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  (None, 4, 50)        20200       repeat_vector_8[0][0]            \n",
      "                                                                 lstm_14[0][1]                    \n",
      "                                                                 lstm_14[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_8 (Dot)                     (None, 4, 10)        0           lstm_15[0][0]                    \n",
      "                                                                 lstm_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 4, 10)        0           dot_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_9 (Dot)                     (None, 4, 50)        0           activation_4[0][0]               \n",
      "                                                                 lstm_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 4, 100)       0           dot_9[0][0]                      \n",
      "                                                                 lstm_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 4, 1)         101         concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 31,301\n",
      "Trainable params: 31,301\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=input_train, outputs=out)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "49/49 [==============================] - 2s 48ms/step - loss: 166.6729 - mae: 10.5921 - val_loss: 112.6744 - val_mae: 9.0704\n",
      "Epoch 2/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 120.6132 - mae: 9.2976 - val_loss: 113.1030 - val_mae: 9.0753\n",
      "Epoch 3/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 116.8677 - mae: 9.2321 - val_loss: 109.4241 - val_mae: 8.9689\n",
      "Epoch 4/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 114.4437 - mae: 9.1208 - val_loss: 111.2138 - val_mae: 9.0416\n",
      "Epoch 5/250\n",
      "49/49 [==============================] - 1s 24ms/step - loss: 111.4978 - mae: 9.0370 - val_loss: 112.0358 - val_mae: 9.0602\n",
      "Epoch 6/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 110.5866 - mae: 8.9989 - val_loss: 110.6046 - val_mae: 9.0036\n",
      "Epoch 7/250\n",
      "49/49 [==============================] - 1s 24ms/step - loss: 110.0304 - mae: 8.9760 - val_loss: 109.3623 - val_mae: 8.9662\n",
      "Epoch 8/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 109.0734 - mae: 8.9679 - val_loss: 109.4581 - val_mae: 8.9714\n",
      "Epoch 9/250\n",
      "49/49 [==============================] - 2s 38ms/step - loss: 108.8061 - mae: 8.9558 - val_loss: 111.4092 - val_mae: 9.0357\n",
      "Epoch 10/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 109.2707 - mae: 8.9805 - val_loss: 109.8824 - val_mae: 8.9909\n",
      "Epoch 11/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 108.4866 - mae: 8.9509 - val_loss: 109.4292 - val_mae: 8.9852\n",
      "Epoch 12/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 108.0942 - mae: 8.9514 - val_loss: 108.4017 - val_mae: 8.9533\n",
      "Epoch 13/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 108.2210 - mae: 8.9393 - val_loss: 105.9644 - val_mae: 8.8567\n",
      "Epoch 14/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 108.0312 - mae: 8.9431 - val_loss: 107.8476 - val_mae: 8.9246\n",
      "Epoch 15/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 108.1790 - mae: 8.9536 - val_loss: 110.8958 - val_mae: 9.0283\n",
      "Epoch 16/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 107.1326 - mae: 8.9160 - val_loss: 108.3826 - val_mae: 8.9424\n",
      "Epoch 17/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 106.6215 - mae: 8.8750 - val_loss: 108.3930 - val_mae: 8.9527\n",
      "Epoch 18/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 106.8325 - mae: 8.9038 - val_loss: 107.7397 - val_mae: 8.9424\n",
      "Epoch 19/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 106.3641 - mae: 8.8733 - val_loss: 107.0131 - val_mae: 8.9159\n",
      "Epoch 20/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 106.6403 - mae: 8.9037 - val_loss: 108.9868 - val_mae: 8.9712\n",
      "Epoch 21/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 106.1218 - mae: 8.8817 - val_loss: 107.5051 - val_mae: 8.9188\n",
      "Epoch 22/250\n",
      "49/49 [==============================] - 1s 21ms/step - loss: 107.1063 - mae: 8.9146 - val_loss: 106.9933 - val_mae: 8.9070\n",
      "Epoch 23/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 105.2579 - mae: 8.8367 - val_loss: 106.1558 - val_mae: 8.8810\n",
      "Epoch 24/250\n",
      "49/49 [==============================] - 1s 24ms/step - loss: 105.6581 - mae: 8.8727 - val_loss: 108.0923 - val_mae: 8.9450\n",
      "Epoch 25/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 105.6820 - mae: 8.8557 - val_loss: 108.5144 - val_mae: 8.9642\n",
      "Epoch 26/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 106.5834 - mae: 8.9024 - val_loss: 107.2268 - val_mae: 8.9190\n",
      "Epoch 27/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 104.9877 - mae: 8.8438 - val_loss: 107.9995 - val_mae: 8.9493\n",
      "Epoch 28/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 105.4907 - mae: 8.8476 - val_loss: 105.0546 - val_mae: 8.8444\n",
      "Epoch 29/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 105.3468 - mae: 8.8570 - val_loss: 106.1283 - val_mae: 8.8748\n",
      "Epoch 30/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 105.2056 - mae: 8.8395 - val_loss: 105.1142 - val_mae: 8.8380\n",
      "Epoch 31/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 105.3165 - mae: 8.8350 - val_loss: 106.3198 - val_mae: 8.8770\n",
      "Epoch 32/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 105.0298 - mae: 8.8358 - val_loss: 106.1124 - val_mae: 8.8711\n",
      "Epoch 33/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 105.0262 - mae: 8.8369 - val_loss: 105.9537 - val_mae: 8.8724\n",
      "Epoch 34/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 105.1863 - mae: 8.8373 - val_loss: 105.9832 - val_mae: 8.8720\n",
      "Epoch 35/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 105.8888 - mae: 8.8827 - val_loss: 104.6822 - val_mae: 8.8299\n",
      "Epoch 36/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 105.0577 - mae: 8.8267 - val_loss: 105.4850 - val_mae: 8.8526\n",
      "Epoch 37/250\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 104.6145 - mae: 8.8276 - val_loss: 106.2948 - val_mae: 8.8802\n",
      "Epoch 38/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 105.4102 - mae: 8.8566 - val_loss: 105.6528 - val_mae: 8.8655\n",
      "Epoch 39/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 105.0182 - mae: 8.8396 - val_loss: 105.2584 - val_mae: 8.8591\n",
      "Epoch 40/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 104.2723 - mae: 8.8154 - val_loss: 105.7208 - val_mae: 8.8759\n",
      "Epoch 41/250\n",
      "49/49 [==============================] - 2s 36ms/step - loss: 104.8799 - mae: 8.8346 - val_loss: 106.3367 - val_mae: 8.8932\n",
      "Epoch 42/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 105.6831 - mae: 8.8530 - val_loss: 106.6937 - val_mae: 8.9025\n",
      "Epoch 43/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 105.3489 - mae: 8.8665 - val_loss: 106.8067 - val_mae: 8.9039\n",
      "Epoch 44/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 104.4987 - mae: 8.8257 - val_loss: 105.6828 - val_mae: 8.8714\n",
      "Epoch 45/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 105.4180 - mae: 8.8582 - val_loss: 107.3833 - val_mae: 8.9262\n",
      "Epoch 46/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 104.9905 - mae: 8.8366 - val_loss: 104.7971 - val_mae: 8.8381\n",
      "Epoch 47/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 105.0062 - mae: 8.8504 - val_loss: 106.6560 - val_mae: 8.8960\n",
      "Epoch 48/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 104.4911 - mae: 8.8265 - val_loss: 105.9814 - val_mae: 8.8808\n",
      "Epoch 49/250\n",
      "49/49 [==============================] - 2s 36ms/step - loss: 104.3930 - mae: 8.8270 - val_loss: 107.8827 - val_mae: 8.9402\n",
      "Epoch 50/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 104.3715 - mae: 8.8203 - val_loss: 107.2261 - val_mae: 8.9231\n",
      "Epoch 51/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 104.6476 - mae: 8.8394 - val_loss: 108.4065 - val_mae: 8.9617\n",
      "Epoch 52/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 103.7792 - mae: 8.8072 - val_loss: 104.9368 - val_mae: 8.8534\n",
      "Epoch 53/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 104.6735 - mae: 8.8172 - val_loss: 106.6764 - val_mae: 8.9068\n",
      "Epoch 54/250\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 104.0584 - mae: 8.7996 - val_loss: 105.5185 - val_mae: 8.8699\n",
      "Epoch 55/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 104.5597 - mae: 8.8251 - val_loss: 105.9641 - val_mae: 8.8808\n",
      "Epoch 56/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 103.8634 - mae: 8.8052 - val_loss: 104.4091 - val_mae: 8.8320\n",
      "Epoch 57/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 104.0843 - mae: 8.8049 - val_loss: 104.9725 - val_mae: 8.8527\n",
      "Epoch 58/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 103.6538 - mae: 8.7925 - val_loss: 105.2703 - val_mae: 8.8590\n",
      "Epoch 59/250\n",
      "49/49 [==============================] - 2s 39ms/step - loss: 104.2923 - mae: 8.8153 - val_loss: 105.1289 - val_mae: 8.8542\n",
      "Epoch 60/250\n",
      "49/49 [==============================] - 2s 37ms/step - loss: 104.6085 - mae: 8.8324 - val_loss: 105.4124 - val_mae: 8.8613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/250\n",
      "49/49 [==============================] - 2s 40ms/step - loss: 103.8998 - mae: 8.7971 - val_loss: 105.8570 - val_mae: 8.8777\n",
      "Epoch 62/250\n",
      "49/49 [==============================] - 2s 41ms/step - loss: 104.0240 - mae: 8.8071 - val_loss: 106.7895 - val_mae: 8.9093\n",
      "Epoch 63/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 104.2727 - mae: 8.8155 - val_loss: 105.1464 - val_mae: 8.8620\n",
      "Epoch 64/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 103.6692 - mae: 8.7922 - val_loss: 105.8685 - val_mae: 8.8809\n",
      "Epoch 65/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 103.6558 - mae: 8.7961 - val_loss: 106.3495 - val_mae: 8.8945\n",
      "Epoch 66/250\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 104.1656 - mae: 8.8094 - val_loss: 104.2043 - val_mae: 8.8280\n",
      "Epoch 67/250\n",
      "49/49 [==============================] - 2s 39ms/step - loss: 103.3396 - mae: 8.7786 - val_loss: 103.6182 - val_mae: 8.8079\n",
      "Epoch 68/250\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 103.8566 - mae: 8.8068 - val_loss: 103.9500 - val_mae: 8.8171\n",
      "Epoch 69/250\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 104.2129 - mae: 8.8330 - val_loss: 104.2547 - val_mae: 8.8309\n",
      "Epoch 70/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 103.8861 - mae: 8.8028 - val_loss: 103.7754 - val_mae: 8.8119\n",
      "Epoch 71/250\n",
      "49/49 [==============================] - 2s 46ms/step - loss: 103.4032 - mae: 8.7741 - val_loss: 105.5897 - val_mae: 8.8794\n",
      "Epoch 72/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 103.7583 - mae: 8.7934 - val_loss: 105.5913 - val_mae: 8.8735\n",
      "Epoch 73/250\n",
      "49/49 [==============================] - 4s 78ms/step - loss: 103.9024 - mae: 8.7989 - val_loss: 106.5094 - val_mae: 8.9051\n",
      "Epoch 74/250\n",
      "49/49 [==============================] - 2s 36ms/step - loss: 103.8724 - mae: 8.8070 - val_loss: 104.3672 - val_mae: 8.8324\n",
      "Epoch 75/250\n",
      "49/49 [==============================] - 3s 56ms/step - loss: 103.7808 - mae: 8.8113 - val_loss: 105.2304 - val_mae: 8.8592\n",
      "Epoch 76/250\n",
      "49/49 [==============================] - 2s 37ms/step - loss: 103.7086 - mae: 8.8028 - val_loss: 106.3296 - val_mae: 8.8985\n",
      "Epoch 77/250\n",
      "49/49 [==============================] - 2s 35ms/step - loss: 103.7975 - mae: 8.7948 - val_loss: 103.9843 - val_mae: 8.8173\n",
      "Epoch 78/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 103.7103 - mae: 8.7904 - val_loss: 106.5276 - val_mae: 8.9060\n",
      "Epoch 79/250\n",
      "49/49 [==============================] - 2s 35ms/step - loss: 103.6539 - mae: 8.7878 - val_loss: 103.8518 - val_mae: 8.8082\n",
      "Epoch 80/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 103.4933 - mae: 8.7887 - val_loss: 103.6252 - val_mae: 8.8001\n",
      "Epoch 81/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 103.8076 - mae: 8.7939 - val_loss: 103.8343 - val_mae: 8.8077\n",
      "Epoch 82/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 102.8285 - mae: 8.7592 - val_loss: 103.3413 - val_mae: 8.7905\n",
      "Epoch 83/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 103.6855 - mae: 8.7918 - val_loss: 103.5038 - val_mae: 8.7986\n",
      "Epoch 84/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 103.3770 - mae: 8.7946 - val_loss: 103.7925 - val_mae: 8.8132\n",
      "Epoch 85/250\n",
      "49/49 [==============================] - 2s 36ms/step - loss: 103.7003 - mae: 8.8058 - val_loss: 103.1259 - val_mae: 8.7788\n",
      "Epoch 86/250\n",
      "49/49 [==============================] - 2s 36ms/step - loss: 102.9765 - mae: 8.7669 - val_loss: 103.1224 - val_mae: 8.7738\n",
      "Epoch 87/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 102.9338 - mae: 8.7552 - val_loss: 104.2888 - val_mae: 8.8233\n",
      "Epoch 88/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 103.7631 - mae: 8.8004 - val_loss: 103.2539 - val_mae: 8.7825\n",
      "Epoch 89/250\n",
      "49/49 [==============================] - 2s 39ms/step - loss: 103.7445 - mae: 8.8004 - val_loss: 104.8248 - val_mae: 8.8413\n",
      "Epoch 90/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 103.2517 - mae: 8.7805 - val_loss: 105.6819 - val_mae: 8.8707\n",
      "Epoch 91/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 103.2998 - mae: 8.7882 - val_loss: 103.5650 - val_mae: 8.7977\n",
      "Epoch 92/250\n",
      "49/49 [==============================] - 2s 37ms/step - loss: 103.1543 - mae: 8.7713 - val_loss: 102.7399 - val_mae: 8.7550\n",
      "Epoch 93/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 103.3426 - mae: 8.7864 - val_loss: 103.7600 - val_mae: 8.8012\n",
      "Epoch 94/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 103.3813 - mae: 8.7903 - val_loss: 103.1035 - val_mae: 8.7749\n",
      "Epoch 95/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 102.8744 - mae: 8.7615 - val_loss: 102.9193 - val_mae: 8.7579\n",
      "Epoch 96/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 103.8806 - mae: 8.8051 - val_loss: 103.9434 - val_mae: 8.8063\n",
      "Epoch 97/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 103.0520 - mae: 8.7713 - val_loss: 102.9718 - val_mae: 8.7701\n",
      "Epoch 98/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 103.8714 - mae: 8.8065 - val_loss: 104.3477 - val_mae: 8.8327\n",
      "Epoch 99/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 103.3353 - mae: 8.7929 - val_loss: 103.5688 - val_mae: 8.8018\n",
      "Epoch 100/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 103.0121 - mae: 8.7674 - val_loss: 104.9682 - val_mae: 8.8491\n",
      "Epoch 101/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 103.0375 - mae: 8.7704 - val_loss: 103.3194 - val_mae: 8.7948\n",
      "Epoch 102/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 102.8503 - mae: 8.7629 - val_loss: 103.1274 - val_mae: 8.7833\n",
      "Epoch 103/250\n",
      "49/49 [==============================] - 2s 35ms/step - loss: 103.0630 - mae: 8.7602 - val_loss: 103.6382 - val_mae: 8.8024\n",
      "Epoch 104/250\n",
      "49/49 [==============================] - 2s 35ms/step - loss: 103.1504 - mae: 8.7730 - val_loss: 103.9056 - val_mae: 8.8149\n",
      "Epoch 105/250\n",
      "49/49 [==============================] - 2s 41ms/step - loss: 103.4069 - mae: 8.7948 - val_loss: 103.3993 - val_mae: 8.7938\n",
      "Epoch 106/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 103.4948 - mae: 8.7960 - val_loss: 103.2423 - val_mae: 8.7904\n",
      "Epoch 107/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 103.0383 - mae: 8.7636 - val_loss: 104.9472 - val_mae: 8.8569\n",
      "Epoch 108/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 103.1763 - mae: 8.7746 - val_loss: 103.1641 - val_mae: 8.7859\n",
      "Epoch 109/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 103.1337 - mae: 8.7717 - val_loss: 103.7322 - val_mae: 8.8066\n",
      "Epoch 110/250\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 103.1120 - mae: 8.7747 - val_loss: 103.6779 - val_mae: 8.8018\n",
      "Epoch 111/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 103.0069 - mae: 8.7808 - val_loss: 103.1882 - val_mae: 8.7806\n",
      "Epoch 112/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 103.0359 - mae: 8.7757 - val_loss: 103.0355 - val_mae: 8.7721\n",
      "Epoch 113/250\n",
      "49/49 [==============================] - 2s 35ms/step - loss: 103.5518 - mae: 8.7985 - val_loss: 104.0811 - val_mae: 8.8205\n",
      "Epoch 114/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 102.7186 - mae: 8.7575 - val_loss: 103.1265 - val_mae: 8.7824\n",
      "Epoch 115/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 103.1149 - mae: 8.7849 - val_loss: 103.9107 - val_mae: 8.8119\n",
      "Epoch 116/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 103.3792 - mae: 8.7813 - val_loss: 103.5422 - val_mae: 8.8010\n",
      "Epoch 117/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 103.1593 - mae: 8.7858 - val_loss: 103.0075 - val_mae: 8.7696\n",
      "Epoch 118/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 103.1319 - mae: 8.7821 - val_loss: 103.7473 - val_mae: 8.8072\n",
      "Epoch 119/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 103.1464 - mae: 8.7731 - val_loss: 104.3912 - val_mae: 8.8322\n",
      "Epoch 120/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 1s 28ms/step - loss: 103.0255 - mae: 8.7753 - val_loss: 103.4929 - val_mae: 8.7966\n",
      "Epoch 121/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 102.9450 - mae: 8.7756 - val_loss: 102.9443 - val_mae: 8.7672\n",
      "Epoch 122/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 102.7844 - mae: 8.7630 - val_loss: 105.1634 - val_mae: 8.8576\n",
      "Epoch 123/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 102.9292 - mae: 8.7623 - val_loss: 103.8186 - val_mae: 8.8091\n",
      "Epoch 124/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 103.1968 - mae: 8.7744 - val_loss: 103.4170 - val_mae: 8.7945\n",
      "Epoch 125/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 102.8183 - mae: 8.7540 - val_loss: 103.9405 - val_mae: 8.8124\n",
      "Epoch 126/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 102.8577 - mae: 8.7672 - val_loss: 102.8874 - val_mae: 8.7456\n",
      "Epoch 127/250\n",
      "49/49 [==============================] - 2s 35ms/step - loss: 102.8030 - mae: 8.7638 - val_loss: 104.2022 - val_mae: 8.8238\n",
      "Epoch 128/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 102.8414 - mae: 8.7642 - val_loss: 102.9311 - val_mae: 8.7616\n",
      "Epoch 129/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 103.0645 - mae: 8.7817 - val_loss: 103.3052 - val_mae: 8.7887\n",
      "Epoch 130/250\n",
      "49/49 [==============================] - 2s 36ms/step - loss: 102.9396 - mae: 8.7668 - val_loss: 103.3021 - val_mae: 8.7909\n",
      "Epoch 131/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 102.7188 - mae: 8.7653 - val_loss: 102.8624 - val_mae: 8.7687\n",
      "Epoch 132/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 102.4974 - mae: 8.7461 - val_loss: 104.2116 - val_mae: 8.8236\n",
      "Epoch 133/250\n",
      "49/49 [==============================] - 2s 42ms/step - loss: 103.5278 - mae: 8.7877 - val_loss: 103.9527 - val_mae: 8.8143\n",
      "Epoch 134/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 103.1203 - mae: 8.7852 - val_loss: 103.1429 - val_mae: 8.7766\n",
      "Epoch 135/250\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 102.4870 - mae: 8.7509 - val_loss: 103.4608 - val_mae: 8.7980\n",
      "Epoch 136/250\n",
      "49/49 [==============================] - 1s 24ms/step - loss: 102.8891 - mae: 8.7578 - val_loss: 103.0825 - val_mae: 8.7738\n",
      "Epoch 137/250\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 102.7794 - mae: 8.7511 - val_loss: 103.0275 - val_mae: 8.7780\n",
      "Epoch 138/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 102.3687 - mae: 8.7370 - val_loss: 103.8491 - val_mae: 8.8123\n",
      "Epoch 139/250\n",
      "49/49 [==============================] - 2s 35ms/step - loss: 102.7244 - mae: 8.7545 - val_loss: 103.1584 - val_mae: 8.7774\n",
      "Epoch 140/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 102.8716 - mae: 8.7716 - val_loss: 104.1374 - val_mae: 8.8271\n",
      "Epoch 141/250\n",
      "49/49 [==============================] - 2s 39ms/step - loss: 103.0377 - mae: 8.7571 - val_loss: 103.1801 - val_mae: 8.7861\n",
      "Epoch 142/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 102.6198 - mae: 8.7570 - val_loss: 103.2289 - val_mae: 8.7872\n",
      "Epoch 143/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 102.9023 - mae: 8.7755 - val_loss: 103.9739 - val_mae: 8.8172\n",
      "Epoch 144/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 102.5672 - mae: 8.7562 - val_loss: 104.9472 - val_mae: 8.8529\n",
      "Epoch 145/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 102.5531 - mae: 8.7492 - val_loss: 103.1859 - val_mae: 8.7778\n",
      "Epoch 146/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 102.6550 - mae: 8.7637 - val_loss: 103.0573 - val_mae: 8.7665\n",
      "Epoch 147/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 102.7774 - mae: 8.7669 - val_loss: 103.9186 - val_mae: 8.8096\n",
      "Epoch 148/250\n",
      "49/49 [==============================] - 3s 53ms/step - loss: 102.8411 - mae: 8.7649 - val_loss: 104.5224 - val_mae: 8.8361\n",
      "Epoch 149/250\n",
      "49/49 [==============================] - 2s 39ms/step - loss: 102.6416 - mae: 8.7606 - val_loss: 103.7036 - val_mae: 8.7973\n",
      "Epoch 150/250\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 102.6123 - mae: 8.7485 - val_loss: 103.3994 - val_mae: 8.7896\n",
      "Epoch 151/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 102.6484 - mae: 8.7628 - val_loss: 103.7092 - val_mae: 8.8060\n",
      "Epoch 152/250\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 102.8394 - mae: 8.7696 - val_loss: 105.0970 - val_mae: 8.8593\n",
      "Epoch 153/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 102.5164 - mae: 8.7417 - val_loss: 103.3195 - val_mae: 8.7771\n",
      "Epoch 154/250\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 102.1217 - mae: 8.7408 - val_loss: 103.6370 - val_mae: 8.7932\n",
      "Epoch 155/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 102.4956 - mae: 8.7454 - val_loss: 103.7827 - val_mae: 8.8055\n",
      "Epoch 156/250\n",
      "49/49 [==============================] - 2s 35ms/step - loss: 102.6042 - mae: 8.7496 - val_loss: 104.9787 - val_mae: 8.8526\n",
      "Epoch 157/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 102.7068 - mae: 8.7494 - val_loss: 105.7425 - val_mae: 8.8803\n",
      "Epoch 158/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 102.6975 - mae: 8.7525 - val_loss: 104.3808 - val_mae: 8.8296\n",
      "Epoch 159/250\n",
      "49/49 [==============================] - 2s 35ms/step - loss: 102.0632 - mae: 8.7273 - val_loss: 103.5193 - val_mae: 8.7782\n",
      "Epoch 160/250\n",
      "49/49 [==============================] - 2s 41ms/step - loss: 102.7840 - mae: 8.7601 - val_loss: 104.9317 - val_mae: 8.8507\n",
      "Epoch 161/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 102.1403 - mae: 8.7302 - val_loss: 103.7459 - val_mae: 8.7966\n",
      "Epoch 162/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 102.5821 - mae: 8.7488 - val_loss: 103.6604 - val_mae: 8.7893\n",
      "Epoch 163/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 102.3085 - mae: 8.7356 - val_loss: 105.8118 - val_mae: 8.8806\n",
      "Epoch 164/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 102.3411 - mae: 8.7345 - val_loss: 103.3655 - val_mae: 8.7908\n",
      "Epoch 165/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 102.0089 - mae: 8.7290 - val_loss: 103.9149 - val_mae: 8.8106\n",
      "Epoch 166/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 102.3339 - mae: 8.7363 - val_loss: 103.7200 - val_mae: 8.7975\n",
      "Epoch 167/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 102.2502 - mae: 8.7306 - val_loss: 105.7366 - val_mae: 8.8813\n",
      "Epoch 168/250\n",
      "49/49 [==============================] - 2s 36ms/step - loss: 103.0276 - mae: 8.7647 - val_loss: 103.3053 - val_mae: 8.7865\n",
      "Epoch 169/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 102.4583 - mae: 8.7432 - val_loss: 103.1933 - val_mae: 8.7700\n",
      "Epoch 170/250\n",
      "49/49 [==============================] - 2s 39ms/step - loss: 102.2989 - mae: 8.7391 - val_loss: 103.2391 - val_mae: 8.7765\n",
      "Epoch 171/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 102.6489 - mae: 8.7420 - val_loss: 104.3323 - val_mae: 8.8277\n",
      "Epoch 172/250\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 102.1426 - mae: 8.7358 - val_loss: 103.7139 - val_mae: 8.8055\n",
      "Epoch 173/250\n",
      "49/49 [==============================] - 2s 35ms/step - loss: 101.8269 - mae: 8.7173 - val_loss: 104.0804 - val_mae: 8.8137\n",
      "Epoch 174/250\n",
      "49/49 [==============================] - 2s 36ms/step - loss: 101.8259 - mae: 8.7204 - val_loss: 103.8744 - val_mae: 8.8082\n",
      "Epoch 175/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 101.7336 - mae: 8.7090 - val_loss: 103.1155 - val_mae: 8.7763\n",
      "Epoch 176/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 102.4594 - mae: 8.7405 - val_loss: 103.0954 - val_mae: 8.7672\n",
      "Epoch 177/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 101.9397 - mae: 8.7195 - val_loss: 104.4896 - val_mae: 8.8350\n",
      "Epoch 178/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 102.3615 - mae: 8.7391 - val_loss: 103.0950 - val_mae: 8.7737\n",
      "Epoch 179/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 1s 30ms/step - loss: 102.1874 - mae: 8.7344 - val_loss: 103.2114 - val_mae: 8.7724\n",
      "Epoch 180/250\n",
      "49/49 [==============================] - 2s 35ms/step - loss: 101.9207 - mae: 8.7157 - val_loss: 103.3682 - val_mae: 8.7713\n",
      "Epoch 181/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 102.0954 - mae: 8.7265 - val_loss: 103.5478 - val_mae: 8.7941\n",
      "Epoch 182/250\n",
      "49/49 [==============================] - 3s 59ms/step - loss: 102.2041 - mae: 8.7461 - val_loss: 104.0830 - val_mae: 8.8225\n",
      "Epoch 183/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 102.6473 - mae: 8.7519 - val_loss: 104.0304 - val_mae: 8.8219\n",
      "Epoch 184/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 102.5932 - mae: 8.7522 - val_loss: 103.4684 - val_mae: 8.7925\n",
      "Epoch 185/250\n",
      "49/49 [==============================] - 2s 39ms/step - loss: 101.8162 - mae: 8.7213 - val_loss: 103.2192 - val_mae: 8.7787\n",
      "Epoch 186/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 102.5283 - mae: 8.7479 - val_loss: 103.1154 - val_mae: 8.7687\n",
      "Epoch 187/250\n",
      "49/49 [==============================] - 2s 35ms/step - loss: 102.3125 - mae: 8.7506 - val_loss: 103.4247 - val_mae: 8.7825\n",
      "Epoch 188/250\n",
      "49/49 [==============================] - 2s 38ms/step - loss: 101.8599 - mae: 8.7257 - val_loss: 104.1203 - val_mae: 8.8175\n",
      "Epoch 189/250\n",
      "49/49 [==============================] - 2s 35ms/step - loss: 101.8763 - mae: 8.7233 - val_loss: 103.9520 - val_mae: 8.8076\n",
      "Epoch 190/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 101.9886 - mae: 8.7330 - val_loss: 103.9144 - val_mae: 8.8089\n",
      "Epoch 191/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 102.3798 - mae: 8.7436 - val_loss: 104.8932 - val_mae: 8.8519\n",
      "Epoch 192/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 102.0885 - mae: 8.7380 - val_loss: 104.9502 - val_mae: 8.8553\n",
      "Epoch 193/250\n",
      "49/49 [==============================] - 2s 37ms/step - loss: 102.2271 - mae: 8.7291 - val_loss: 103.6906 - val_mae: 8.8043\n",
      "Epoch 194/250\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 101.9038 - mae: 8.7106 - val_loss: 104.5138 - val_mae: 8.8308\n",
      "Epoch 195/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 101.9717 - mae: 8.7177 - val_loss: 103.9515 - val_mae: 8.8127\n",
      "Epoch 196/250\n",
      "49/49 [==============================] - 2s 36ms/step - loss: 102.4160 - mae: 8.7458 - val_loss: 103.4596 - val_mae: 8.7942\n",
      "Epoch 197/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 102.2917 - mae: 8.7364 - val_loss: 103.2717 - val_mae: 8.7753\n",
      "Epoch 198/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 101.7851 - mae: 8.7013 - val_loss: 103.4557 - val_mae: 8.7855\n",
      "Epoch 199/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 102.3224 - mae: 8.7309 - val_loss: 104.7145 - val_mae: 8.8367\n",
      "Epoch 200/250\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 101.8054 - mae: 8.7252 - val_loss: 102.9649 - val_mae: 8.7510\n",
      "Epoch 201/250\n",
      "49/49 [==============================] - 2s 49ms/step - loss: 102.2168 - mae: 8.7343 - val_loss: 105.0054 - val_mae: 8.8483\n",
      "Epoch 202/250\n",
      "49/49 [==============================] - 2s 38ms/step - loss: 102.0272 - mae: 8.7176 - val_loss: 103.2388 - val_mae: 8.7787\n",
      "Epoch 203/250\n",
      "49/49 [==============================] - 2s 37ms/step - loss: 101.3952 - mae: 8.6979 - val_loss: 102.9312 - val_mae: 8.7637\n",
      "Epoch 204/250\n",
      "49/49 [==============================] - 2s 39ms/step - loss: 102.2663 - mae: 8.7384 - val_loss: 103.3395 - val_mae: 8.7828\n",
      "Epoch 205/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 101.8020 - mae: 8.7235 - val_loss: 103.0239 - val_mae: 8.7639\n",
      "Epoch 206/250\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 102.0284 - mae: 8.7225 - val_loss: 102.8935 - val_mae: 8.7548\n",
      "Epoch 207/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 101.7279 - mae: 8.7090 - val_loss: 103.4127 - val_mae: 8.7847\n",
      "Epoch 208/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 101.8436 - mae: 8.7080 - val_loss: 103.4813 - val_mae: 8.7839\n",
      "Epoch 209/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 101.9910 - mae: 8.7237 - val_loss: 104.2225 - val_mae: 8.8182\n",
      "Epoch 210/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 102.2221 - mae: 8.7263 - val_loss: 103.7781 - val_mae: 8.7959\n",
      "Epoch 211/250\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 102.0025 - mae: 8.7262 - val_loss: 103.6833 - val_mae: 8.7918\n",
      "Epoch 212/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 102.2855 - mae: 8.7189 - val_loss: 103.7622 - val_mae: 8.7958\n",
      "Epoch 213/250\n",
      "49/49 [==============================] - 2s 40ms/step - loss: 101.7504 - mae: 8.7107 - val_loss: 103.2399 - val_mae: 8.7702\n",
      "Epoch 214/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 102.1020 - mae: 8.7144 - val_loss: 103.1598 - val_mae: 8.7828\n",
      "Epoch 215/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 101.8355 - mae: 8.7213 - val_loss: 104.6703 - val_mae: 8.8395\n",
      "Epoch 216/250\n",
      "49/49 [==============================] - 2s 42ms/step - loss: 101.5603 - mae: 8.7077 - val_loss: 103.2409 - val_mae: 8.7709\n",
      "Epoch 217/250\n",
      "49/49 [==============================] - 2s 40ms/step - loss: 102.4798 - mae: 8.7458 - val_loss: 104.3350 - val_mae: 8.7976\n",
      "Epoch 218/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 101.5541 - mae: 8.6976 - val_loss: 103.9065 - val_mae: 8.8072\n",
      "Epoch 219/250\n",
      "49/49 [==============================] - 2s 38ms/step - loss: 102.1115 - mae: 8.7372 - val_loss: 103.9690 - val_mae: 8.8117\n",
      "Epoch 220/250\n",
      "49/49 [==============================] - 4s 82ms/step - loss: 101.9632 - mae: 8.7045 - val_loss: 104.6006 - val_mae: 8.8382\n",
      "Epoch 221/250\n",
      "49/49 [==============================] - 2s 37ms/step - loss: 101.7118 - mae: 8.6998 - val_loss: 104.1453 - val_mae: 8.8170\n",
      "Epoch 222/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 101.9887 - mae: 8.7183 - val_loss: 104.5029 - val_mae: 8.8374\n",
      "Epoch 223/250\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 101.3776 - mae: 8.6975 - val_loss: 103.1400 - val_mae: 8.7783\n",
      "Epoch 224/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 101.4054 - mae: 8.7055 - val_loss: 104.7852 - val_mae: 8.8493\n",
      "Epoch 225/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 102.0488 - mae: 8.7291 - val_loss: 103.6962 - val_mae: 8.7961\n",
      "Epoch 226/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 101.7765 - mae: 8.7086 - val_loss: 104.0502 - val_mae: 8.8104\n",
      "Epoch 227/250\n",
      "49/49 [==============================] - 1s 27ms/step - loss: 101.7763 - mae: 8.7166 - val_loss: 104.2987 - val_mae: 8.8262\n",
      "Epoch 228/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 101.7039 - mae: 8.7148 - val_loss: 104.2764 - val_mae: 8.8274\n",
      "Epoch 229/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 101.3168 - mae: 8.7010 - val_loss: 103.6167 - val_mae: 8.7863\n",
      "Epoch 230/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 101.3821 - mae: 8.6967 - val_loss: 103.8974 - val_mae: 8.7898\n",
      "Epoch 231/250\n",
      "49/49 [==============================] - 1s 31ms/step - loss: 101.3538 - mae: 8.6827 - val_loss: 103.9858 - val_mae: 8.8124\n",
      "Epoch 232/250\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 101.5897 - mae: 8.7054 - val_loss: 103.9422 - val_mae: 8.8097\n",
      "Epoch 233/250\n",
      "49/49 [==============================] - 2s 35ms/step - loss: 101.5868 - mae: 8.7070 - val_loss: 103.9095 - val_mae: 8.8046\n",
      "Epoch 234/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 101.0701 - mae: 8.6730 - val_loss: 103.7425 - val_mae: 8.8010\n",
      "Epoch 235/250\n",
      "49/49 [==============================] - 2s 36ms/step - loss: 101.4416 - mae: 8.6973 - val_loss: 103.7060 - val_mae: 8.7941\n",
      "Epoch 236/250\n",
      "49/49 [==============================] - 2s 35ms/step - loss: 101.7017 - mae: 8.7002 - val_loss: 103.9851 - val_mae: 8.8074\n",
      "Epoch 237/250\n",
      "49/49 [==============================] - 2s 36ms/step - loss: 101.6042 - mae: 8.7003 - val_loss: 103.5295 - val_mae: 8.7886\n",
      "Epoch 238/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 2s 34ms/step - loss: 101.4162 - mae: 8.7001 - val_loss: 103.7510 - val_mae: 8.7887\n",
      "Epoch 239/250\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 101.4097 - mae: 8.6796 - val_loss: 104.0853 - val_mae: 8.8136\n",
      "Epoch 240/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 101.9141 - mae: 8.7260 - val_loss: 103.3934 - val_mae: 8.7822\n",
      "Epoch 241/250\n",
      "49/49 [==============================] - 2s 36ms/step - loss: 101.7181 - mae: 8.7063 - val_loss: 103.0732 - val_mae: 8.7721\n",
      "Epoch 242/250\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 101.8131 - mae: 8.7010 - val_loss: 103.4799 - val_mae: 8.7903\n",
      "Epoch 243/250\n",
      "49/49 [==============================] - 2s 34ms/step - loss: 101.2892 - mae: 8.6914 - val_loss: 103.9014 - val_mae: 8.7967\n",
      "Epoch 244/250\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 101.3430 - mae: 8.6733 - val_loss: 104.9032 - val_mae: 8.8470\n",
      "Epoch 245/250\n",
      "49/49 [==============================] - 2s 35ms/step - loss: 101.3619 - mae: 8.6831 - val_loss: 104.2201 - val_mae: 8.8252\n",
      "Epoch 246/250\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 101.4575 - mae: 8.6947 - val_loss: 103.7271 - val_mae: 8.8012\n",
      "Epoch 247/250\n",
      "49/49 [==============================] - 2s 42ms/step - loss: 101.6904 - mae: 8.7064 - val_loss: 104.3490 - val_mae: 8.8300\n",
      "Epoch 248/250\n",
      "49/49 [==============================] - 1s 29ms/step - loss: 101.6137 - mae: 8.6993 - val_loss: 103.3039 - val_mae: 8.7852\n",
      "Epoch 249/250\n",
      "49/49 [==============================] - 2s 38ms/step - loss: 100.8787 - mae: 8.6788 - val_loss: 103.6530 - val_mae: 8.8036\n",
      "Epoch 250/250\n",
      "49/49 [==============================] - 2s 36ms/step - loss: 101.2076 - mae: 8.6856 - val_loss: 103.4213 - val_mae: 8.7897\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, validation_split=0.25, \n",
    "                    epochs=epochs, verbose=1, callbacks=[es], \n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_mae = history.history['mae']\n",
    "valid_mae = history.history['val_mae'] \n",
    "#model.save('model_forecasting_seq2seq.h5')\n",
    "\n",
    "plt.plot(train_mae, label='train mae'), \n",
    "plt.plot(valid_mae, label='validation mae')\n",
    "plt.ylabel('mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('train vs. validation accuracy (mae)')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=False, ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting last reserved data point:\n",
      "True:      [25 27 20 17]\n",
      "Predicted: [[17.]\n",
      " [19.]\n",
      " [17.]\n",
      " [16.]]\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "print('Predicting last reserved data point:')\n",
    "print('True:     ', goldeny)\n",
    "print('Predicted:', predict_one( model, goldenx ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on train set...\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 100.1655 - mae: 8.6465\n",
      "\n",
      "Train:  100.16551208496094\n",
      "Train MSE:     8.646524429321289\n",
      "Predicting on last 100 rows of train set:\n",
      "\n",
      "True:      [19  5  4 28]\n",
      "Predicted:[[19.]\n",
      " [16.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [33 22  3 30]\n",
      "Predicted:[[18.]\n",
      " [16.]\n",
      " [16.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [23  2 18  3]\n",
      "Predicted:[[17.]\n",
      " [14.]\n",
      " [15.]\n",
      " [15.]]\n",
      "============================\n",
      "True:      [23  1 15 29]\n",
      "Predicted:[[17.]\n",
      " [19.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [10  7 18 33]\n",
      "Predicted:[[18.]\n",
      " [19.]\n",
      " [18.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [18 13  4 35]\n",
      "Predicted:[[18.]\n",
      " [18.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [27 11 13 32]\n",
      "Predicted:[[18.]\n",
      " [17.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [19 23  5 28]\n",
      "Predicted:[[18.]\n",
      " [18.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [23 24 17 34]\n",
      "Predicted:[[18.]\n",
      " [16.]\n",
      " [16.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [30 26  8 20]\n",
      "Predicted:[[18.]\n",
      " [17.]\n",
      " [16.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [ 8 30 11  3]\n",
      "Predicted:[[17.]\n",
      " [17.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [ 6 12 19  5]\n",
      "Predicted:[[16.]\n",
      " [18.]\n",
      " [18.]\n",
      " [19.]]\n",
      "============================\n",
      "True:      [31 18  5  3]\n",
      "Predicted:[[18.]\n",
      " [18.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [ 8  5  3 10]\n",
      "Predicted:[[19.]\n",
      " [19.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [ 4 18 23  9]\n",
      "Predicted:[[17.]\n",
      " [18.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [ 3 18 24 14]\n",
      "Predicted:[[18.]\n",
      " [19.]\n",
      " [19.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [24  8 26 10]\n",
      "Predicted:[[18.]\n",
      " [20.]\n",
      " [19.]\n",
      " [19.]]\n",
      "============================\n",
      "True:      [21  1 24 13]\n",
      "Predicted:[[17.]\n",
      " [19.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [15 34 30  9]\n",
      "Predicted:[[18.]\n",
      " [19.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [22  1 27 15]\n",
      "Predicted:[[18.]\n",
      " [18.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [23  6 32 27]\n",
      "Predicted:[[19.]\n",
      " [19.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [11  5 31 16]\n",
      "Predicted:[[19.]\n",
      " [18.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [18  2 19 25]\n",
      "Predicted:[[18.]\n",
      " [19.]\n",
      " [16.]\n",
      " [15.]]\n",
      "============================\n",
      "True:      [14 18 24 17]\n",
      "Predicted:[[18.]\n",
      " [20.]\n",
      " [16.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [ 4 30  1 22]\n",
      "Predicted:[[19.]\n",
      " [20.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [ 7 15  1 21]\n",
      "Predicted:[[18.]\n",
      " [18.]\n",
      " [18.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [32 25 23 30]\n",
      "Predicted:[[19.]\n",
      " [18.]\n",
      " [18.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [ 5  6 25 29]\n",
      "Predicted:[[19.]\n",
      " [17.]\n",
      " [17.]\n",
      " [15.]]\n",
      "============================\n",
      "True:      [13 32  9 29]\n",
      "Predicted:[[19.]\n",
      " [19.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [25 22 18 19]\n",
      "Predicted:[[20.]\n",
      " [18.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [ 5 25 22 28]\n",
      "Predicted:[[21.]\n",
      " [18.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [34 30 25 20]\n",
      "Predicted:[[20.]\n",
      " [20.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [ 4 17 14 18]\n",
      "Predicted:[[19.]\n",
      " [17.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [35 20 23 17]\n",
      "Predicted:[[19.]\n",
      " [18.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [ 4  1 34 13]\n",
      "Predicted:[[19.]\n",
      " [17.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [23 15 28  1]\n",
      "Predicted:[[18.]\n",
      " [19.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [27 29  3  8]\n",
      "Predicted:[[19.]\n",
      " [18.]\n",
      " [16.]\n",
      " [15.]]\n",
      "============================\n",
      "True:      [ 3 24 28 26]\n",
      "Predicted:[[18.]\n",
      " [15.]\n",
      " [18.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [ 4  8 11 27]\n",
      "Predicted:[[17.]\n",
      " [18.]\n",
      " [18.]\n",
      " [19.]]\n",
      "============================\n",
      "True:      [10 11 21 28]\n",
      "Predicted:[[18.]\n",
      " [18.]\n",
      " [18.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [16 10 13 20]\n",
      "Predicted:[[18.]\n",
      " [17.]\n",
      " [16.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [34 35  9 29]\n",
      "Predicted:[[18.]\n",
      " [17.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [19  3 27 29]\n",
      "Predicted:[[17.]\n",
      " [15.]\n",
      " [15.]\n",
      " [14.]]\n",
      "============================\n",
      "True:      [ 4 19 14 23]\n",
      "Predicted:[[20.]\n",
      " [18.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [13 32  2 34]\n",
      "Predicted:[[20.]\n",
      " [18.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [35  5  7 30]\n",
      "Predicted:[[19.]\n",
      " [18.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [ 1 28 32  4]\n",
      "Predicted:[[19.]\n",
      " [18.]\n",
      " [15.]\n",
      " [14.]]\n",
      "============================\n",
      "True:      [11 34 29 25]\n",
      "Predicted:[[19.]\n",
      " [19.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [ 1 15 18 22]\n",
      "Predicted:[[18.]\n",
      " [18.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [12 35 25  4]\n",
      "Predicted:[[18.]\n",
      " [19.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [21  7  9 14]\n",
      "Predicted:[[17.]\n",
      " [17.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [ 4  9 21 19]\n",
      "Predicted:[[18.]\n",
      " [17.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [20 12 14 34]\n",
      "Predicted:[[17.]\n",
      " [18.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [29 34  6 23]\n",
      "Predicted:[[17.]\n",
      " [18.]\n",
      " [16.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [ 6 13  9 27]\n",
      "Predicted:[[17.]\n",
      " [16.]\n",
      " [16.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [29 26 10  1]\n",
      "Predicted:[[16.]\n",
      " [17.]\n",
      " [16.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [23 17  9 19]\n",
      "Predicted:[[17.]\n",
      " [17.]\n",
      " [16.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [15 28 32 24]\n",
      "Predicted:[[16.]\n",
      " [16.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [11  5  8 10]\n",
      "Predicted:[[18.]\n",
      " [18.]\n",
      " [17.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [21 24 28  4]\n",
      "Predicted:[[18.]\n",
      " [18.]\n",
      " [18.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [ 6 18  4 19]\n",
      "Predicted:[[18.]\n",
      " [18.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [28 32 15  8]\n",
      "Predicted:[[18.]\n",
      " [17.]\n",
      " [18.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [26 32 10 21]\n",
      "Predicted:[[17.]\n",
      " [17.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [ 3  2 18  7]\n",
      "Predicted:[[18.]\n",
      " [16.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [19 14 20 15]\n",
      "Predicted:[[18.]\n",
      " [18.]\n",
      " [18.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [32 21  4 29]\n",
      "Predicted:[[19.]\n",
      " [18.]\n",
      " [18.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [34  4 13 21]\n",
      "Predicted:[[19.]\n",
      " [17.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [25 23  9 29]\n",
      "Predicted:[[19.]\n",
      " [17.]\n",
      " [18.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [13  5 16 30]\n",
      "Predicted:[[17.]\n",
      " [17.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [ 8 26 20  3]\n",
      "Predicted:[[19.]\n",
      " [20.]\n",
      " [19.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [14 25  4  1]\n",
      "Predicted:[[17.]\n",
      " [19.]\n",
      " [18.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [34  9 22  3]\n",
      "Predicted:[[18.]\n",
      " [19.]\n",
      " [18.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [31 32 26  2]\n",
      "Predicted:[[18.]\n",
      " [19.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [ 7 33 25  5]\n",
      "Predicted:[[17.]\n",
      " [18.]\n",
      " [17.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [ 2 26 11 35]\n",
      "Predicted:[[18.]\n",
      " [18.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [19 12 15  7]\n",
      "Predicted:[[16.]\n",
      " [16.]\n",
      " [16.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [20 12 16 33]\n",
      "Predicted:[[16.]\n",
      " [16.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [ 6  1 25 11]\n",
      "Predicted:[[16.]\n",
      " [16.]\n",
      " [16.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [23 17 21  1]\n",
      "Predicted:[[16.]\n",
      " [17.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [ 4 31  8  1]\n",
      "Predicted:[[15.]\n",
      " [17.]\n",
      " [16.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [29 26 23 21]\n",
      "Predicted:[[16.]\n",
      " [17.]\n",
      " [17.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [ 5 18  1 28]\n",
      "Predicted:[[17.]\n",
      " [16.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [32  4  6 12]\n",
      "Predicted:[[18.]\n",
      " [17.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [29  4 24 20]\n",
      "Predicted:[[19.]\n",
      " [16.]\n",
      " [16.]\n",
      " [15.]]\n",
      "============================\n",
      "True:      [25 15  1 21]\n",
      "Predicted:[[17.]\n",
      " [19.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [25 17 20 12]\n",
      "Predicted:[[18.]\n",
      " [16.]\n",
      " [16.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [18  8  1 12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:[[17.]\n",
      " [17.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [26 25 18 23]\n",
      "Predicted:[[17.]\n",
      " [16.]\n",
      " [17.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [10 14  6  1]\n",
      "Predicted:[[16.]\n",
      " [16.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [18 26 11 30]\n",
      "Predicted:[[16.]\n",
      " [17.]\n",
      " [16.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [19  9 22  5]\n",
      "Predicted:[[18.]\n",
      " [17.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [25 35  7 30]\n",
      "Predicted:[[18.]\n",
      " [18.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [13  2 26 24]\n",
      "Predicted:[[18.]\n",
      " [16.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [ 3 15 22 30]\n",
      "Predicted:[[20.]\n",
      " [19.]\n",
      " [19.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [30  3 11 19]\n",
      "Predicted:[[20.]\n",
      " [19.]\n",
      " [19.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [28  6  3 30]\n",
      "Predicted:[[20.]\n",
      " [19.]\n",
      " [18.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [ 8  9 28  3]\n",
      "Predicted:[[19.]\n",
      " [18.]\n",
      " [16.]\n",
      " [16.]]\n",
      "============================\n",
      "True:      [34 15 12 27]\n",
      "Predicted:[[19.]\n",
      " [20.]\n",
      " [18.]\n",
      " [18.]]\n",
      "============================\n",
      "True:      [ 7 29 22 26]\n",
      "Predicted:[[18.]\n",
      " [18.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n",
      "True:      [32  4 30  3]\n",
      "Predicted:[[16.]\n",
      " [17.]\n",
      " [17.]\n",
      " [17.]]\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "def predict_on_trainset( model, X_, y_ ):\n",
    "    \n",
    "    for feature_set, target in zip( X_, y_ ):\n",
    "        print('True:     ', target)\n",
    "        print('Predicted:', predict_one( model, feature_set ), '\\n', '='*28, sep='')\n",
    "                \n",
    "        \n",
    "# evaluate\n",
    "print('Evaluating on train set...')\n",
    "score = model.evaluate( X, y, verbose=1 )\n",
    "print('\\nTrain: ', score[0])\n",
    "print('Train MSE:    ', score[1])\n",
    "\n",
    "# predict\n",
    "m = 100\n",
    "print(f'Predicting on last {m} rows of train set:\\n')\n",
    "predict_on_trainset( model, X[-m:], y[-m:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unknown_variable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-84c84b8d1b57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munknown_variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'unknown_variable' is not defined"
     ]
    }
   ],
   "source": [
    "unknown_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix. Code from DL notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model & plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras LR Schedules: https://keras.io/api/optimizers/learning_rate_schedules/\n",
    "# KERAS WAY TO DEFINE LR SCHEDULES\n",
    "#initial_learning_rate = 0.001\n",
    "#lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(   initial_learning_rate,\n",
    "#                                                                decay_steps=500,         # 1 epoch = 50 steps\n",
    "#                                                                decay_rate=0.96,\n",
    "#                                                                staircase=True )\n",
    "\n",
    "#model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule),\n",
    "#              loss='sparse_categorical_crossentropy',\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "#model.fit(data, labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR WAY TO DEFINE LR DECAY\n",
    "import math\n",
    "def step_decay(epoch):\n",
    "    \n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.5\n",
    "    epochs_drop = 50.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "        \n",
    "    return lrate\n",
    "\n",
    "\n",
    "def exponential_decay(epoch):\n",
    "    \n",
    "    initial_lrate = 1.0\n",
    "    k = 0.7\n",
    "    lrate = initial_lrate * math.exp(-k*epoch)\n",
    "        \n",
    "    return lrate\n",
    "\n",
    "# USAGE\n",
    "#if use_lr_decay:\n",
    "#    lrate = LearningRateScheduler( step_decay )\n",
    "#    callbacks = [lrate]\n",
    "#else:\n",
    "#    callbacks = []\n",
    "#model.fit( ... callbacks=callbacks )\n",
    "\n",
    "for i in range(10):\n",
    "    print( step_decay(i) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loss:        mse, mae, huber, logcosh    (mape, msle - maximize mse; need input data scaling?)\n",
    "# optimizers: SGD, RMSprop, Adagrad, Adadelta, Adamax, Adam, Nadam, Ftrl\n",
    "# metrics:    metric_mse, metric_rmse, metric_mae, metric_mape, metric_msle, metric_lcosh\n",
    "\n",
    "# model parameters\n",
    "initial_learning_rate  = 0.001\n",
    "units         = 40\n",
    "dropout1      = 0.075\n",
    "dropout2      = 0\n",
    "dropout3      = 0\n",
    "loss          = mse\n",
    "optimizer     = Adamax\n",
    "use_time_series = True\n",
    "use_lr_decay  = False\n",
    "metrics       = [ metric_lcosh, metric_mae, metric_msle, ]\n",
    "\n",
    "# if lr decay - Keras API\n",
    "if use_lr_decay:    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(   initial_learning_rate,\n",
    "                                                                    decay_steps=500,       # 1 epoch = 50 steps\n",
    "                                                                    decay_rate=0.96,\n",
    "                                                                    staircase=True )\n",
    "    learning_rate_to_model = lr_schedule\n",
    "else:\n",
    "    learning_rate_to_model = initial_learning_rate\n",
    "\n",
    "# initiate model\n",
    "model_name = 'multi_stacked_LSTM'                                      # used further in plots\n",
    "model = multi_stacked_LSTM( n_steps, n_features,\n",
    "                        learning_rate_ = learning_rate_to_model,\n",
    "                        units = units,\n",
    "                        dropout1 = dropout1,\n",
    "                        dropout2 = dropout2,\n",
    "                        dropout3 = dropout3,\n",
    "                        loss=loss,\n",
    "                        optimizer=optimizer,\n",
    "                        use_lr_decay=use_lr_decay,\n",
    "                        metrics=metrics,\n",
    "                      )\n",
    "\n",
    "# use unshuffled (time series) or shuffled data\n",
    "if use_time_series:\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "else:\n",
    "    X_train = X_sh\n",
    "    y_train = y_sh\n",
    "do_shuffle = not use_time_series                                   # shuffling while fitting\n",
    "\n",
    "# model fit paramaters\n",
    "batch_size = 32\n",
    "epochs     = 250\n",
    "verbose    = 1\n",
    "\n",
    "# learning rate & early stopping\n",
    "lrate = LearningRateScheduler( step_decay )\n",
    "callbacks = [ EarlyStopping( monitor='loss',\n",
    "                             min_delta=0.001,\n",
    "                             patience=20,\n",
    "                             verbose=verbose,\n",
    "                             restore_best_weights=True,          # restore from the best previous epoch\n",
    "                             mode='min'                         #(stops decreaqsing) or 'max'; default 'auto'\n",
    "                           ),\n",
    "#              lrate,\n",
    "            ]\n",
    "                          \n",
    "# fit model\n",
    "history = model.fit( X_train, y_train,\n",
    "                     batch_size=batch_size,\n",
    "                     epochs=epochs,\n",
    "                     shuffle=do_shuffle,\n",
    "                     verbose=verbose,\n",
    "                     validation_split=0.25,\n",
    "                     callbacks=callbacks, )                      #see lr_schedule w/optimizer above for lr_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT RESULTS (Note: ncols & nrows are constant values for all subplots)\n",
    "optimizer_name = model.optimizer.get_config()['name']\n",
    "loss_func_name = model.loss.get_config()['name']\n",
    "metric_names   = [ i.get_config()['name'] for i in model.metrics if i.get_config()['name'] != 'loss']\n",
    "features_list  = ', '.join(features)\n",
    "\n",
    "ncols = 2\n",
    "nrows = (len(metric_names) + 1) // ncols\n",
    "figure_width  = 15\n",
    "figure_height = nrows*0.75*(15/ncols) - 0.5\n",
    "plt.subplots(nrows, ncols, figsize=(figure_width, figure_height))\n",
    "joint_title = model_name + f' (batch_size {batch_size}, epochs {epochs}, loss {loss_func_name}, ' +\\\n",
    "              f'lr {initial_learning_rate}, units {units}, optimizer {optimizer_name},\\nn_steps {n_steps}, ' +\\\n",
    "              f'n_features {n_features}, dropout1={dropout1}, dropout2={dropout2}, dropout3={dropout3},\\n' +\\\n",
    "              f'use_time_series={use_time_series}, with_intersection={with_intersection}, ' +\\\n",
    "              f'flatten={flatten}, random_state={random_state},\\nfeatures: {features_list})'\n",
    "                \n",
    "plt.suptitle( joint_title, fontsize=14 )\n",
    "\n",
    "# remove very large values in the beginning\n",
    "start_value = 2\n",
    "\n",
    "plt.subplot(nrows, ncols, 1)\n",
    "plt.plot(history.history['loss'][start_value:])\n",
    "plt.plot(history.history['val_loss'][start_value:])\n",
    "plt.ylabel(loss_func_name + ' loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train','val'], loc='upper right')\n",
    "#plt.show()\n",
    "\n",
    "for idx, metric_name in enumerate(metric_names):\n",
    "    plt.subplot(nrows, ncols, idx+2)\n",
    "    plt.plot(history.history[metric_name][start_value:])\n",
    "    plt.plot(history.history['val_' + metric_name][start_value:])\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(['train','val'], loc='upper right')\n",
    "        \n",
    "plt.tight_layout()\n",
    "\n",
    "# save figure\n",
    "if use_time_series:\n",
    "    ts_indicator = '_TS_'\n",
    "else:\n",
    "    ts_indicator = '_Shuffle_'\n",
    "timestr   = time.strftime('%Y%m%d-%H%M%S')\n",
    "folder    = './plots/'\n",
    "file_name = folder + model_name + '_' + loss_func_name.capitalize() + '_' + optimizer_name + '_' + str(epochs) + '_Epochs' + ts_indicator + timestr + '.jpeg'\n",
    "plt.savefig(file_name)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( min(history.history['val_loss']) )\n",
    "min_val = min(history.history['val_loss'])\n",
    "print(history.history['val_loss'].index(min_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr   = time.strftime('%Y%m%d-%H%M%S')\n",
    "folder    = './models/'\n",
    "file_name = folder + timestr + '_' + model_name + '_' + loss_func_name.capitalize() + '_' + optimizer_name + '_' + str(epochs) + '_Epochs' + ts_indicator + str(n_steps) + 'n_steps' + '.h5'\n",
    "model.save(file_name)\n",
    "\n",
    "# LOAD MODEL\n",
    "#from keras.models import load_model\n",
    "#del model\n",
    "#model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on last reserved data point and partial train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "print('Predicting last reserved data point:')\n",
    "print('True:     ', goldeny)\n",
    "print('Predicted:', predict_one( model, goldenx ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EVALUATE AND PREDICT ON TRAIN SET\n",
    "def predict_on_trainset( model, X_, y_ ):\n",
    "    \n",
    "    for feature_set, target in zip( X_, y_ ):\n",
    "        print('True:     ', target)\n",
    "        print('Predicted:', predict_one( model, feature_set ), '\\n', '='*28, sep='')\n",
    "                \n",
    "        \n",
    "# evaluate\n",
    "print('Evaluating on train set...')\n",
    "score = model.evaluate( X_train, y_train, verbose=verbose )\n",
    "print('\\nTrain ' + loss_func_name.capitalize() + ':', score[0])\n",
    "print('Train MSE:    ', score[1], '\\n\\n', '='*100, '\\n', sep='')\n",
    "\n",
    "# predict\n",
    "m = 100\n",
    "print(f'Predicting on last {m} rows of train set:\\n')\n",
    "predict_on_trainset( model, X_train[-m:], y_train[-m:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OPTIMIZERS. IMPORTED TO BE CALLABLE ( Adam(lr=learning_rate) )__  \n",
    "Using the code below does not make them callable. Each optimizer has also `**kwargs` as a parameter:\n",
    "* `SGD = tf.keras.optimizers.SGD( learning_rate=0.01, momentum=0.0, nesterov=False, name=\"SGD\" )    # old`\n",
    "* `RMSprop = tf.keras.optimizers.RMSprop( learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False, name=\"RMSprop\" )`\n",
    "* `Adagrad = tf.keras.optimizers.Adagrad( learning_rate=0.001, initial_accumulator_value=0.1, epsilon=1e-07,name=\"Adagrad\" )`\n",
    "* `Adadelta = tf.keras.optimizers.Adadelta( learning_rate=0.001, rho=0.95, epsilon=1e-07, name=\"Adadelta\" )`\n",
    "* `Adamax = tf.keras.optimizers.Adamax( learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\" )`\n",
    "* `Adam = tf.keras.optimizers.Adam( learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name=\"Adam\" )`\n",
    "* `Nadam = tf.keras.optimizers.Nadam( learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\" )`\n",
    "* `Ftrl = tf.keras.optimizers.Ftrl( learning_rate=0.001, learning_rate_power=-0.5, initial_accumulator_value=0.1, l1_regularization_strength=0.0, l2_regularization_strength=0.0, name=\"Ftrl\", l2_shrinkage_regularization_strength=0.0 )`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
